
\chapter{Appendix}
\label{cha:appendix}

This chapter contains all the missing details from the previous chapters,
especially the proofs, experiment descriptions, and other relevant information.
This is done to ensure that the thesis is concise for all readers, and for
readers who are interested in low-level details they can refer to this chapter
as needed.

\section{Appendix for Chapter~\ref{CHA:ARS}}
\label{sec:append-chapt-ars}

\subsection{Proof of Theorem~\ref{thm:online_linear_regression}}
\label{sec:proofs_bandit}
\begin{proof}[Proof of Theorem~\ref{thm:online_linear_regression}]
%Result in Eq~\ref{eq:ogd} is directly from \cite{Zinkevich2003_ICML} with the fact that $\|w\|_2\leq\mathcal{W}$ and $\|\nabla_{w}\ell_t(w)\|_2\leq C\mathcal{X}$ to any $w$ and $t$.

To prove Eq.~\ref{eq:random_para} for Alg.~\ref{alg:random_search_OLR}, we use the proof techniques from \cite{flaxman2005online}. The proof is more simpler than the one in \cite{flaxman2005online} as we do not have to deal with shrinking and reshaping the predictor set ${\Theta}$.

Denote $u\sim \mathbb{B}_b$ as uniformly sampling $u$ from a $b$-dim unit ball, $u\sim\mathbb{S}_b$ as uniformly sampling $u$ from the $b$-dim unit sphere, and $\delta \in (0,1)$. Consider the loss function $\hat{c}_i(w_i) = \mathbb{E}_{v\sim \mathbb{B}_b}[c_i(\theta_i + \delta v)]$, which is a smoothed version of $c_i(w_i)$. It is shown in \cite{flaxman2005online} that the gradient of $\hat{c}_i$ with respect to $\theta$ is:
\begin{align*}
   &\nabla_{\theta}\hat{c}_i(\theta)|_{\theta = \theta_i} \\
   &= \frac{b}{\delta} \mathbb{E}_{u\sim\mathbb{S}_b}[c_i(\theta_i +\delta u)u]\\
   &= \frac{b}{\delta}\mathbb{E}_{u\sim \mathbb{S}_b}[((\theta_i +\delta u)^T s_i - a_i)^2 u].
\end{align*} Hence, the descent direction we take in Alg.~\ref{alg:random_search_OLR} is actually an unbiased estimate of $\nabla_{\theta}\hat{c}_i(\theta)|_{\theta=\theta_i}$. So Alg.~\ref{alg:random_search_OLR} can be considered as running OGD with an unbiased estimate of gradient on the sequence of loss $\hat{c}_i(\theta_i)$. It is not hard to show that for an unbiased estimate of $\nabla_{\theta}\hat{c}_i(\theta)|_{\theta=\theta_i}$ = $\frac{b}{\delta} ((\theta_i + \delta u)^T s_i - a_i)^2 u$, the norm is bounded as $b(C^2 + C_{s}^2)/\delta$. Now we can directly applying Lemma 3.1 from \cite{flaxman2005online}, to get:
\begin{align}
\label{eq:regret_on_surrogate}
   \mathbb{E}\left[\sum_{i=1}^T \hat{c}_i(\theta_i)\right] - \min_{\theta^\star\in{\Theta}}\sum_{i=1}^T \hat{c}_i(\theta^\star) \leq \frac{C_{\theta}b(C^2+C_{s}^2)}{\delta}\sqrt{T}.
\end{align} We can bound the difference between $\hat{c}_i(\theta)$ and ${c}_i(\theta)$ using the Lipschitiz continuous property of $c_i$:
\begin{align}
|\hat{c}_i(\theta) - c_i(\theta) | & = |\mathbb{E}_{v\sim \mathbb{B}_b}[c_i(\theta+\delta v) - c_i(\theta)]| \nonumber\\
&\leq \mathbb{E}_{v\sim \mathbb{B}_b}[|c_i(\theta+\delta v) - c_i(\theta)|] \leq L\delta.
\end{align} Substitute the above inequality back to Eq.~\ref{eq:regret_on_surrogate}, rearrange terms, we get:
\begin{align}
&\mathbb{E}\left[ \sum_{i=1}^T c_i(\theta_i)  \right]  - \min_{\theta^\star\in{\Theta}} \sum_{i=1}^T c_i(w^\star)\leq \frac{C_{\theta}b(C^2+C_{s}^2)}{\delta}\sqrt{T} + 2LT\delta.
\end{align} By setting $\delta = T^{-0.25}\sqrt{\frac{C_{\theta}b(C^2+C_{s}^2)}{2L}}$, we get:
\begin{align*}
   &\mathbb{E}\left[ \sum_{i=1}^T c_i(\theta_i)  \right]  - \min_{w^\star\in{\Theta}} \sum_{i=1}^T c_i(w^\star) \leq \sqrt{C_{\theta}b(C^2+C_{s}^2)L} T^{3/4}.
\end{align*}

To prove Eq.~\ref{eq:random_action} for Alg.~\ref{alg:random_search_action}, we follow the similar strategy in the proof of Alg.~\ref{alg:random_search_OLR}.

Denote $\epsilon \sim [-1,1]$ as uniformly sampling $\epsilon$ from the interval $[-1,1]$, $e\sim \{-1,1\}$ as uniformly sampling $e$ from the set containing $-1$ and $1$. Consider the loss function $\tilde{c}_i(\theta) = \mathbb{E}_{\epsilon\sim [-1,1]}[(\theta^T s_i + \delta \epsilon - a_i)^2]$. One can show that the gradient of $\tilde{c}_i(\theta)$ with respect to $\theta$ is:
\begin{align}
    \nabla_{\theta}\tilde{c}_i(\theta) = \frac{1}{\delta}\mathbb{E}_{e\sim \{-1,1\}}[e(\theta^{\top} s_i + \delta e - a_i)^2 s_i].
\end{align} As we can see that the descent direction we take in Alg.~\ref{alg:random_search_action} is actually an unbiased estimate of $\nabla_{\theta}\tilde{c}_i(\theta)|_{\theta=\theta_i}$. Hence Alg.~\ref{alg:random_search_action} can be considered as running OGD with unbiased estimates of gradients on the sequence of loss functions $\tilde{c}_i(\theta)$. For an unbiased estimate of the gradient, $\frac{1}{\delta} e(\theta_i^{\top} s_i +\delta e - a_i)^2 s_i$, its norm is bounded as $(C^2 + 1)C_{s}/\delta$. Note that different from Alg.~\ref{alg:random_search_OLR}, here the maximum norm of the unbiased gradient \emph{is independent of feature dimension $b$}. Now we apply Lemma 3.1 from \cite{flaxman2005online} on $\tilde{c}_i$, to get:
\begin{align}
\label{eq:tilde_random_action}
    \mathbb{E}\left[ \sum_{i=1}^T \tilde{c}_i(\theta_i)\right] - \min_{\theta^\star\in{\Theta}}\sum_{i=1}^T \tilde{c}_i(\theta^*) \leq \frac{C_{\theta}(C^2 + 1)C_{s}}{\delta}\sqrt{T}.
\end{align}
Again we can bound the difference between $\tilde{c}_i(\theta)$ and $c_i(\theta)$ for any $\theta$ using the fact that $(\hat{a}_i - a_i)^2$ is Lipschitz continuous with respect to prediction $\hat{a}_i$ with Lipschitz constant $C$:
\begin{align}
    |\tilde{c}_i(\theta) - c_i(\theta)| &= |\mathbb{E}_{\epsilon\sim [-1,1]} [(\theta^{\top} s_i + \delta\epsilon - a_i)^2 - (\theta^{\top} s_i - a_i)^2]|  \nonumber\\
    &\leq \mathbb{E}_{\epsilon\sim [-1,-1]}[C\delta |\epsilon|] \leq C\delta.
\end{align} Substitute the above inequality back to Eq.~\ref{eq:tilde_random_action}, rearrange terms:
\begin{align*}
    &\mathbb{E}\left[\sum_{i=1}^T \tilde{c}_i(\theta_i)\right] - \min_{\theta^\star\in{\Theta}}\sum_{i=1}^T \tilde{c}_i(\theta^*)\leq \frac{C_{\theta}(C^2+1)C_{s}}{\delta}\sqrt{T} + 2C\delta T.
\end{align*}
Set $\delta = T^{-0.25}\sqrt{\frac{C_{\theta}(C^2+1)C_{s}}{2C}}$, we get:
\begin{align*}
  &\mathbb{E}\left[\sum_{i=1}^T \tilde{c}_i(\theta_i)\right] - \min_{\theta^*\in{\Theta}}\sum_{i=1}^T \tilde{c}_i(\theta^*)\leq \sqrt{C_{\theta}(C^2+1)C_{s}C}T^{3/4}.
\end{align*}
\end{proof}




\subsection{Proof of Theorem~\ref{theorem:parameter-convergence}}
\label{sec:proofs_RL}

We first present some useful lemmas below.


Consider the smoothed objective given by $\hat{J}(\theta) =
\mathbb{E}_{v \sim \mathbb{B}_d}[J(\theta + \delta v)]$ where
$\mathbb{B}_d$ is the unit ball in $d$ dimensions and $\delta$ is a
positive constant. Using the assumptions stated in Section
\ref{sec:assumptions_parameter}, we obtain the following useful lemma:
\begin{lemma}
  \label{lemma:grad-diff-parameter}
  If the objective $J(\theta)$ satisfies the assumptions in Section
  \ref{sec:assumptions_parameter} and the smoothed objective
  $\hat{J}(\theta)$
  is as given above,
  %is given by $\hat{J}(\theta) = \mathbb{E}_{v \sim
  %  \mathbb{B}_d}[J(\theta + \delta v)]$ where $\delta > 0$ and
  %  $\mathbb{B}_d$ is the unit ball in $d$ dimensions
  then we have that
  \begin{enumerate}
  \item $\hat{J}(\theta)$ is also $G$-Lipschitz and $L$-smooth
  \item For all $\theta \in \mathbb{R}^d$, $\|\nabla_\theta J(\theta)
    - \nabla_\theta \hat{J}(\theta)\| \leq L\delta$
  \end{enumerate}
\end{lemma}



\begin{proof}[Proof of Lemma \ref{lemma:grad-diff-parameter}]
  Consider for any $\theta_1, \theta_2 \in \mathbb{R}^d$,
\begin{align*}
    |\hat{J}(\theta_1) - \hat{J}(\theta_2)| &= |\mathbb{E}_{v \sim \mathbb{B}_d}[J(\theta_1+\delta v) - J(\theta_2 + \delta v)]| \nonumber \\
    &\leq \mathbb{E}_{v \sim \mathbb{B}_d}[|J(\theta_1+\delta v) - J(\theta_2 + \delta v)|] \nonumber \\
    &\leq \mathbb{E}_{v \sim \mathbb{B}_d}[G\|\theta_1 - \theta_2\|] \nonumber \\
    &= G\|\theta_1 - \theta_2\|
\end{align*}
The above inequalities are due to the fact that expectation of absolute value is greater than absolute value of expectation, and the $G$-lipschitz assumption on $J(\theta)$. Thus, the smoothened loss function $\hat{J}(\theta)$ is also $G$-lipschitz. Similarly consider,
\begin{align*}
  \|\nabla_\theta\hat{J}&(\theta_1) - \nabla_\theta\hat{J}(\theta_2)\| \\
  &= \|\nabla_\theta \mathbb{E}_{v \sim \mathbb{B}_d}[J(\theta_1 + \delta v)] - \nabla_\theta \mathbb{E}_{v \sim \mathbb{B}_d}[J(\theta_2+\delta v)]\| \nonumber \\
    &= \|\mathbb{E}_{v \sim \mathbb{B}_d}[\nabla_\theta J(\theta_1+\delta v)
      - \nabla_\theta J(\theta_2 + \delta v)]\| \nonumber \\
    &\leq \mathbb{E}_{v \sim \mathbb{B}_d}[\|\nabla_\theta J(\theta_1+\delta
      v) - \nabla_\theta J(\theta_2 + \delta v)\|] \nonumber \\
    &\leq \mathbb{E}_{v \sim \mathbb{B}_d}[L\|\theta_1 - \theta_2\|] \nonumber \\
    &= L\|\theta_1 - \theta_2\|
\end{align*}
The above inequalities are due to the fact that expectation of norm is
greater than norm of expectation, and the $L$-smoothness assumption on
$J(\theta_1)$. We interchange the expectation and derivative using the
assumptions on $J(\theta_1)$ and the dominated convergence
theorem. Thus, the smoothened loss function $\hat{J}(\theta_1)$ is
also $L$-smooth.


We know,
  \begin{align*}
    \nabla_\theta \hat{J}(\theta) &= \nabla_\theta\mathbb{E}_{v \sim \mathbb{B}_d}[J(\theta +
                             \delta v)] \nonumber \\
    &= \mathbb{E}_{v \sim \mathbb{B}_d}[\nabla_\theta J(\theta + \delta v)]
  \end{align*}
  Note that the expectation and derivative can be interchanged using
  the dominated convergence theorem. Hence, we have
  \begin{align*}
    \|\nabla_\theta \hat{J}(\theta) - \nabla_\theta J(\theta)\| &= \|\mathbb{E}_{u \sim
                                                  \mathbb{B}_d}[\nabla_\theta
                                                  J(\theta + \delta v)]
                                                  - \nabla_\theta J(\theta)\|
                                                  \nonumber \\
                                                &\leq \mathbb{E}_{u \sim
                                                  \mathbb{B}_d}\|\nabla_\theta
                                                  J(\theta + \delta v) -
                                                  \nabla_\theta J(\theta)\|
                                                  \nonumber \\
                                                &\leq \mathbb{E}_{u
                                                  \sim \mathbb{B}_d}[L
                                                  ||\delta v||]
                                                  \nonumber \\
                                                &\leq L \delta
  \end{align*}
\end{proof}

The above lemma will be very useful later when we try to relate the
convergence rate for the smoothed objective and the true objective. It is shown in
\citep{flaxman2005online, agarwal2010optimal} that the gradient estimate $g_i$ is an
unbiased estimator of the gradient $\nabla_\theta
\hat{J}(\theta_i)$. Hence, Algorithm \ref{alg:random_search_parameter}
is performing SGD on the smoothed objective $\hat{J}(\theta)$. Using
this insight, we can use the convergence rate of SGD for nonconvex
functions to stationary points from \citep{ghadimi2013stochastic} which is given as
follows
\begin{lemma}[\citep{ghadimi2013stochastic}]
  \label{lemma:sgd-parameter}
  Consider running SGD on the objective $\hat{J}(\theta)$ that is
  $L$-smooth and $G$-Lipschitz for $T$ steps. Fix initial solution
  $\theta_0$ and denote $\Delta_0 = \hat{J}(\theta_0) -
  \hat{J}(\theta^*)$ where $\theta^*$ is the point at which
  $\hat{J}(\theta)$ attains global minimum. Also, assume that the
  gradient estimate $g_i$ is unbiased and has a bounded variance,
  i.e. for all $i$, $\mathbb{E}_i[\|g_i - \nabla_\theta
  \hat{J}(\theta_i)\|_2^2] \leq V \in \mathbb{R}^+$ where
  $\mathbb{E}_i$ denotes expectation with randomness only at iteration
  $i$ conditioned on history upto iteration $i-1$. Then we have,
  \begin{equation}
    %\label{eq:sgd-parameter}
    \frac{1}{T} \sum_{i=1}^T \mathbb{E}\|\nabla_\theta
    \hat{J}(\theta_i)\|_2^2 \leq \frac{2\sqrt{2\Delta_0L(V+G^2)}}{\sqrt{T}}
  \end{equation}
\end{lemma}
For completeness, we include a proof of the above lemma below.
\begin{proof}[Proof of Lemma \ref{lemma:sgd-parameter}]
  Denote $\xi_i = g_i - \nabla_\theta {\hat{J}}(\theta_i)$.  Note that $\mathbb{E}_{i} [\xi_i] =
0$ since the stochastic gradient $g_i$ is unbiased.
From  $\theta_{i+1} = \theta_i - \alpha g_i$, we have:
\begin{align*}
  \hat{J}(\theta_{i+1}) & = \hat{J}(\theta_{i} - \alpha g_i)\\
  &\leq\hat{J}(\theta_i) - \nabla_\theta \hat{J}(\theta_i)^{\top} (\alpha g_i) + \frac{L\alpha^2}{2}\| g_i\|_2^2  \\
    & = \hat{J}(\theta_i) - \alpha \nabla_\theta \hat{J}(\theta_i)^{\top} g_i + \frac{L\alpha^2}{2} \|\xi_i + \nabla_\theta \hat{J}(\theta_i)\|^2_2 \\
    & = \hat{J}(\theta_i) - \alpha \nabla_\theta
      \hat{J}(\theta_i)^{\top} g_i + \frac{L\alpha^2}{2}(\|\xi_i\|_2^2
  + 2\xi_i^{\top}\nabla_\theta \hat{J}(\theta_i) + \|\nabla_\theta \hat{J}(\theta_i)\|_2^2 )
\end{align*} The first inequality above is obtained since the loss
function $\hat{J}(\theta)$ is $L$-smooth. Adding $\mathbb{E}_i$ on both sides and using the fact that $\mathbb{E}_i [\xi_i] = 0$, we have:
\begin{align*}
    \mathbb{E}_i [\hat{J}(\theta_{i+1})] &= \hat{J}(\theta_i) - \alpha
                                           \|\nabla_\theta
                                           \hat{J}(\theta_i)\|_2^2  +\frac{L\alpha^2}{2}\left( \mathbb{E}_i [\|\xi_i\|_2^2] + \|\nabla_\theta \hat{J}(\theta_i)\|_2^2  \right)  \\
    &\leq \hat{J}(\theta_i) - \alpha \|\nabla_\theta
      \hat{J}(\theta_i)\|_2^2 + \frac{L\alpha^2}{2}\left( \mathbb{E}_i [\|\xi_i\|_2^2] + G^2  \right)
\end{align*}
where the inequality is due to the lipschitz assumption. Rearranging terms, we get:
\begin{align*}
    \alpha\|\nabla_\theta \hat{J}(\theta_i)\|_2^2 &= \hat{J}(\theta_i)
      - \mathbb{E}_i [\hat{J}(\theta_{i+1})] + \frac{L\alpha^2}{2} (\mathbb{E}_i [\|\xi_i\|_2^2] + G^2) \\
    & \leq \hat{J}(\theta_i) - \mathbb{E}_i[ \hat{J}(\theta_{i+1})] + \frac{L\alpha^2}{2} (V + G^2)
\end{align*}
Sum over from time step $1$ to $T$, we get:
\begin{align*}
    \alpha \sum_{t=1}^T \mathbb{E}\|\nabla_\theta
  \hat{J}(\theta_i)\|_2^2 &\leq \mathbb{E} [\hat{J}(\theta_0) -
                            \hat{J}(\theta_T)] + \frac{LT\alpha^2}{2}(V+G^2)
\end{align*} Divide $\alpha$  on both sides, we get:
\begin{align*}
    \sum_{t=1}^{T} \mathbb{E}&\|\nabla_\theta \hat{J}(\theta_i)\|_2^2 \leq \frac{1}{\alpha} \mathbb{E}[\hat{J}(\theta_0) - \hat{J}(\theta_T)] + {LT\alpha} (V+G^2) \\
    & \leq \frac{1}{\alpha} \mathbb{E}[\hat{J}(\theta_0) - \hat{J}(\theta^*)] + {LT\alpha} (V+G^2)  \\
    & = \frac{1}{\alpha} \Delta_0 + {LT\alpha} (V+G^2)  \\
    & \leq \sqrt{\frac{\Delta_0LT(V+G^2)}{2}} + \sqrt{2\Delta_0
      LT(V+G^2)} \\
      &\leq 2\sqrt{2\Delta_0 LT(V+G^2)}
\end{align*} with $\alpha = \sqrt{\frac{2\Delta_0}{LT(V+G^2)}}$.
Hence, we have:
\begin{align*}
    \frac{1}{T}\sum_{t=1}^T\mathbb{E}\|\nabla_\theta \hat{J}(\theta_i)\|_2^2 \leq \frac{2\sqrt{2\Delta_0 L(V+G^2)}}{\sqrt{T}}
\end{align*}
\end{proof}



The above lemma is useful as it gives us the following result:
\begin{align}
  \label{eq:stationary-point}
  \min_{1 \leq i \leq T} \mathbb{E}\|\nabla_\theta\hat{J}(\theta_i)\|_2^2 &\leq \frac{1}{T} \sum_{i=1}^T \mathbb{E}\|\nabla_\theta
                                                               \hat{J}(\theta_i)\|_2^2
                                                               \nonumber
  \\
                                                             &\leq \frac{2\sqrt{2\Delta_0L(V+G^2)}}{\sqrt{T}}
\end{align}
since the minimum is always less than the average. We have then that
using SGD to minimize a nonconvex objective finds a $\theta_i$ that is
`almost' a stationary point in bounded number of steps provided the
stochastic gradient estimate has bounded variance.

We now show that the gradient estimate $g_i$ used in Algorithm
\ref{alg:random_search_parameter} indeed has a bounded variance. Observe that
the estimate $g_i$ in the algorithm is a two-point estimate, which
should have substantially less variance than one-point
estimates \citep{agarwal2010optimal}. However, the two evaluations, resulting in $J_i^+$ and
$J_i^-$, have different independent noise. This is due to the
fact that in policy search, stochasticity arises from the
environment and cannot be controlled and we cannot obtain the
significant variance reduction that is typical of two-point
estimators. The following lemma quantifies the bound on the variance of
gradient estimate $g_i$:
\begin{lemma}
\label{lemma:grad-variance-parameter}
  Consider a smoothed objective $\hat{J}(\theta) = \mathbb{E}_{v \sim
    \mathbb{B}_d}[J(\theta + \delta v)]$ where $\mathbb{B}_d$ is the
  unit ball in $d$ dimensions, $\delta > 0$ is a scalar and the true
  objective $J(\theta)$ is $G$-lipschitz. Given gradient estimate $g_i
  = \frac{d(J_i^+ - J_i^-)}{2\delta}u$ where $u$ is sampled uniformly
  from a unit sphere $\mathbb{S}_d$ in $d$ dimensions, $J^+_i =
  J(\theta_i + \delta u) + \eta^+_i$ and $J_i^- = J(\theta - \delta u)
  + \eta_i^-$ for zero mean random i.i.d noises $\eta_i^+, \eta_i^-$, we have
  \begin{equation}
    \label{eq:grad-variance-parameter}
    \mathbb{E}_i[\|g_i - \nabla_\theta \hat{J}(\theta_i)\|_2^2] \leq
    2d^2G^2 + 2\frac{d^2\sigma^2}{\delta^2}
  \end{equation}
  where {$\sigma^2$ is the variance of the random noise $\eta$.}
\end{lemma}


\begin{proof}[Proof of Lemma \ref{lemma:grad-variance-parameter}]
  From \cite{shamir2017optimal}, we know that $g_i$ is an unbiased estimate of the gradient of $\hat{J}(\theta_i)$, i.e. $\mathbb{E}_{u_i \sim \mathbb{S}_d}[g_i] = \nabla\hat{J}(\theta_i)$. Thus, we have
\begin{align*}
    \mathbb{E}_{u_i \sim \mathbb{S}_d}&\|g_i -
      \nabla\hat{J}(\theta_i)\|^2 \\
      &= \mathbb{E}_{u_i \sim \mathbb{S}_d}[\|g_i\|^2 + \|\nabla \hat{J}(\theta)_i\|^2 - 2g_i^T\nabla \hat{J}(\theta_i)] \\
    &= \mathbb{E}_{u_i \sim \mathbb{S}_d}\|g_i\|^2 + \|\nabla \hat{J}(\theta_i)\|^2 - 2\|\nabla \hat{J}(\theta_i)\|^2 \\
    &= \mathbb{E}_{u_i \sim \mathbb{S}_d}\|g_i\|^2 - \|\nabla \hat{J}(\theta_i)\|^2 \\
    &\leq \mathbb{E}_{u_i \sim \mathbb{S}_d}\|g_i\|^2 \\
    &= \frac{d^2}{4\delta^2}\mathbb{E}_{u_i \sim
      \mathbb{S}_d}\|(J(\theta_i + \delta u_i) - J(\theta_i - \delta
      u_i) + (\eta_i^+ - \eta_i^-))u_i\|^2 \\
  &\leq \frac{d^2}{2\delta^2}[\mathbb{E}_{u_i \sim \mathbb{S}_d}\|(J(\theta_i + \delta u_i) - J(\theta_i - \delta
    u_i)u_i\|_2^2 + \mathbb{E}_{u_i \sim \mathbb{S}_d}\|(\eta_i^+ -
    \eta_i^-))u_i\|^2] \\
    &\leq \frac{d^2}{2\delta^2}[\mathbb{E}_{u_i \sim \mathbb{S}_d}
      4G^2\delta^2 \|u_i\|^2 +  4\mathbb{E}_{u_i \sim
      \mathbb{S}_d}\|\eta_i^+\|_2^2 \|u_i\|_2^2]\\
    &= 2d^2G^2  + 2\frac{d^2\sigma^2}{\delta^2}
\end{align*}
where the second inequality is true as $\|a+b\|_2^2 \leq 2(\|a\|_2^2 +
\|b\|_2^2)$ and the last inequality is due to the Lipschitz assumption
on $J(\theta)$.
\end{proof}

We are ready to prove Theorem~\ref{theorem:parameter-convergence}.
\begin{proof}[Proof of Theorem \ref{theorem:parameter-convergence}]
  Fix initial solution $\theta_0$ and denote $\Delta_0 =
  \hat{J}(\theta_0) - \hat{J}(\theta^*)$ where $\hat{J}(\theta)$ is
  the smoothed objective and $\theta^*$ is the point at which
  $\hat{J}(\theta)$  attains global minimum.
  Since the gradient estimate $g_i$ used in Algorithm
  \ref{alg:random_search_parameter} is an unbiased estimate of the
  gradient $\nabla_\theta \hat{J}(\theta_i)$, we know that Algorithm
  \ref{alg:random_search_parameter} performs SGD on the smoothed
  objective. Moreover, from Lemma \ref{lemma:grad-variance-parameter},
  we know that the variance of the gradient estimate $g_i$ is
  bounded. Hence, we can use Lemma \ref{lemma:sgd-parameter} on the
  smoothed objective $\hat{J}(\theta)$ to get
  \begin{align}
    \label{eq:sgd-parameter}
    \frac{1}{T} \sum_{i=1}^T \mathbb{E}\|\nabla_\theta
    \hat{J}(\theta_i)\|_2^2 \leq \frac{2\sqrt{2\Delta_0L(V+G^2)}}{\sqrt{T}}
  \end{align}
  where $V \leq 2d^2G^2 + 2\frac{d^2\sigma^2}{\delta^2}$ (from Lemma
  \ref{lemma:grad-variance-parameter}). We can relate $\nabla_\theta
  \hat{J}(\theta)$ and $\nabla_\theta J(\theta)$ - the quantity that
  we ultimately care about, as follows:
  \begin{align*}
    \frac{1}{T} &\sum_{i=1}^T \mathbb{E}\|\nabla_\theta
    J(\theta_i)\|_2^2\\
    &= \frac{1}{T} \sum_{i=1}^T
                        \mathbb{E}\|\nabla_\theta J(\theta_i) -
                        \nabla_\theta \hat{J}(\theta_i) +
      \nabla_\theta \hat{J}(\theta_i)\|_2^2 \\
    &\leq \frac{2}{T} \sum_{i=1}^T \mathbb{E}\|\nabla_\theta J(\theta_i) -
                        \nabla_\theta \hat{J}(\theta_i)\|_2^2 + \mathbb{E}\|\nabla_\theta \hat{J}(\theta_i)\|_2^2
  \end{align*}
  We can use Lemma \ref{lemma:grad-diff-parameter} to bound the first
  term and Equation \ref{eq:sgd-parameter} to bound the second
  term. Thus, we have
  \begin{align*}
    \frac{1}{T} \sum_{i=1}^T \mathbb{E}\|\nabla_\theta
    J(\theta_i)\|_2^2 \leq \frac{2}{T}[TL^2\delta^2 + 2\sqrt{2\Delta_0L(V+G^2)T}]
  \end{align*}
  Substituting the bound for $V$ from Lemma
  \ref{lemma:grad-variance-parameter}, using the inequality
  $\sqrt{a+b} \leq \sqrt{a} + \sqrt{b}$ for $a, b \in \mathbb{R}^+$,
  optimizing over $\delta$, and using $\Delta_0 \leq \Qbound$ we get
  \begin{equation*}
    \frac{1}{T} \sum_{i=1}^T \mathbb{E}\|\nabla_\theta
    J(\theta_i)\|_2^2 \leq \mathcal{O}(\Qbound^{\frac{1}{2}}dT^{\frac{-1}{2}} + \Qbound^{\frac{1}{3}}d^{\frac{2}{3}}T^{\frac{-1}{3}}\sigma)
  \end{equation*}
\end{proof}

\subsection{Proof of Theorem~\ref{theorem:action-convergence}}
\label{sec:proof-action-convergence}

The bound on the bias of the gradient estimate is given
by the following lemma:
\begin{lemma}
  \label{lemma:bias-bound-action}
  If the assumptions in Section \ref{sec:assumptions_action} are
  satisfied, then for  the gradient estimate $g_i$ used in Algorithm
  \ref{alg:random_search_action} and the gradient of the objective
  $J(\theta)$ given in equation \ref{eq:dpg-gradient}, we have
  \begin{equation}
    \label{eq:bias-bound-action}
    \|\mathbb{E}[g_i] - \nabla_\theta J(\theta_i)\| \leq KUH\delta
  \end{equation}
\end{lemma}


\begin{proof}[Proof of Lemma \ref{lemma:bias-bound-action}]
  To prove that the bias is bounded, let's consider for any $i$
  \begin{align*}
      &\|\mathbb{E}[g_i] - \nabla_\theta J(\theta_i)\|_2 = \|\sum_{t=0}^{H-1} \mathbb{E}_{s_t\sim
        d_{\pi_{\theta_i}}^t}[\nabla_\theta \pi(\theta_i, s_t) \nabla_a (\mathbb{E}_{v \sim \mathbb{B}_p}
        Q_{\pi_{\theta_i}}^t(s_t, \pi(\theta_i, s_t) + \delta v) -
          Q_{\pi_{\theta_i}}^t(s_t, \pi(\theta_i, s_t)))]\|_2 \\
    &\leq \sum_{t=0}^{H-1} \mathbb{E}_{s_t\sim d_{\pi_{\theta_i}}^t, v
      \sim \mathbb{B}_p}\|\nabla_\theta \pi(\theta_i, s_t)\|_2 \|[\nabla_a
      Q_{\pi_{\theta_i}}^t(s_t, \pi(\theta_i, s_t) + \delta v) -
      \nabla_a Q_{\pi_{\theta_i}}^t(s_t, \pi(\theta_i, s_t))]\|_2 \\
    &\leq \sum_{t=0}^{H-1} KU\delta \mathbb{E}_{v \sim
      \mathbb{B}_p}\|v\|_2 \\
    &\leq KUH\delta
  \end{align*}
  The first inequality above is obtained by using the fact that
  $\|\mathbb{E}[X]\|_2 \leq \mathbb{E}\|X\|_2$, and the second
  inequality using the $K$-lipschitz assumption on $\pi(\theta, s)$
  and $U$-smooth assumption on $Q_{\pi_\theta}^t(s, a)$ in $a$. Also,
  observe that we interchanged the derivative and expectation above by
  using the assumptions on $Q_{\pi_\theta}^t$ as stated in Section
  \ref{sec:assumptions_action}.
\end{proof}

We will now show that the gradient estimate $g_i$ used in Algorithm
\ref{alg:random_search_action} has a bounded variance. Note that the
gradient estimate constructed in Algorithm
\ref{alg:random_search_action} is a one-point estimate, unlike policy
search in parameter space where we had a two-point estimate. Thus, the variance would be higher and the bound
on the variance of such a one-point estimate is given below
\begin{lemma}
  \label{lemma:grad-variance-action}
  Given a gradient estimate $g_i$ as shown in Algorithm
  \ref{alg:random_search_action}, the variance of the estimate can be
  bounded as
  \begin{equation}
    \label{eq:grad-variance-action}
    \mathbb{E}\|g_i - \mathbb{E}[g_i]\|_2^2 \leq
    \frac{2H^2p^2K^2}{\delta^2} ((\Qbound + W\delta)^2 + \sigma^2)
  \end{equation}
  where $\sigma^2$ is the variance of the random noise $\tilde{\eta}$.
\end{lemma}



\begin{proof}[Proof of Lemma \ref{lemma:grad-variance-action}]
  To bound the variance of the gradient estimate $g_i$ in Algorithm
  \ref{alg:random_search_action}, lets consider
  \begin{align*}
    &\mathbb{E}_i\|g_i - \mathbb{E}[g_i]\|_2^2 = \mathbb{E}_i\|g_i\|_2^2 -
                                              \|\mathbb{E}_i[g_i]\|_2^2
                                            \leq \mathbb{E}_i\|g_i\|_2^2 \\
    &= \frac{H^2p^2}{\delta^2} \mathbb{E}_i\|\nabla_\theta
      \pi(\theta_i, s_t)
    (Q_{\pi_{\theta_i}}^t(s_t, \pi(\theta_i, s_t)
      + \delta u) + \tilde{\eta}_i) u\|_2^2 \\
    &\leq \frac{K^2p^2H^2}{\delta^2} \mathbb{E}_{i}\|Q_{\pi_{\theta_i}}^t(s_t, \pi(\theta_i,
      s_t) + \delta u)u + \tilde{\eta}_iu\|_2^2
  \end{align*}
  where $\mathbb{E}_i$ denotes expectation with respect to the
  randomness at iteration $i$ and the inequality is obtained using
  $K$-lipschitz assumption on $\pi(\theta, s)$. Note that we can
  express $Q_{\pi_{\theta_i}}^t(s_t, \pi(\theta_i, s_t) + \delta u)
  \leq Q_{\pi_{\theta_i}}^t(s_t, \pi(\theta_i, s_t)) + W\delta\|u\|_2
  \leq \Qbound + W\delta$ where we used the $W$-lipschitz assumption on
  $Q_{\pi_{\theta}}^t(s, a)$ in $a$ and that it is bounded everywhere
  by constant $\Qbound$. Thus, we have
  \begin{align*}
    &\mathbb{E}_i\|g_i - \mathbb{E}[g_i]\|_2^2 \\
    &\leq \frac{K^2p^2H^2}{\delta^2} \mathbb{E}_{i}\|(\Qbound + W\delta)u +
      \tilde{\eta}_iu\|_2^2 \\
    &\leq \frac{2K^2p^2H^2}{\delta^2}
      (\mathbb{E}_i\|(\Qbound+W\delta)u\|_2^2 +
      \mathbb{E}_i\|\tilde{\eta}_iu\|_2^2 \\
    &\leq \frac{2K^2p^2H^2}{\delta^2} ((\Qbound+W\delta)^2 + \sigma^2)
  \end{align*}
\end{proof}

We are now ready to prove theorem \ref{theorem:action-convergence}
\begin{proof}[Proof of Theorem \ref{theorem:action-convergence}]
  Fix initial solution $\theta_0$ and denote $\Delta_0 =
  J(\theta_0) - J(\theta^*)$ where $\theta^*$ is the point at which
  $J(\theta)$ attains global minimum. Denote $\xi_i = g_i - \mathbb{E}_i[g_i]$ and $\beta_i =
  \mathbb{E}_i[g_i] - \nabla_\theta J(\theta_i)$. From Lemma
  \ref{lemma:bias-bound-action}, we know $\|\beta_i\| \leq KUH\delta$
  and from lemma \ref{lemma:grad-variance-action}, we know
  $\mathbb{E}\|\xi_i\|_2^2 = V \leq \frac{2K^2p^2H^2}{\delta^2} ((\Qbound +
  W\delta)^2 + \sigma^2)$ and $\mathbb{E}_i[\xi_i] = 0$ from definition. From $\theta_{i+1} = \theta_i - \alpha g_i$
  we have:
  \begin{align*}
    J(\theta_{i+1}) &= J(\theta_i - \alpha g_i) \\
    &\leq J(\theta_i) - \alpha \nabla_\theta J(\theta_i)^Tg_i +
      \frac{L\alpha^2}{2}\|g_i\|_2^2 \\
    &= J(\theta_i) - \alpha \nabla_\theta J(\theta_i)^T g_i +
      \frac{L\alpha^2}{2}\|\xi_i + \mathbb{E}_i[g_i]\|_2^2 \\
                    &= J(\theta_i) - \alpha \nabla_\theta J(\theta_i)^T g_i +
      \frac{L\alpha^2}{2}(\|\mathbb{E}_i[g_i]\|_2^2 + \|\xi_i\|_2^2 + 2\mathbb{E}_i[g_i]^T\xi_i)
  \end{align*}
  Taking expectation on both sides with respect to randomness at
  iteration $i$, we have
  \begin{align*}
    &\mathbb{E}_i[J(\theta_{i+1})] = J(\theta_i) - \alpha\nabla_\theta
    J(\theta_i)^T\mathbb{E}_i[g_i] + \frac{L\alpha^2}{2}(\|\mathbb{E}_i [g_i]\|_2^2 +
    \mathbb{E}_i\|\xi_i\|_2^2 +
      2\mathbb{E}_i[g_i]^T\mathbb{E}_i[\xi_i]) \\
    &\leq J(\theta_i) - \alpha\nabla_\theta J(\theta_i)^T (\beta_i +
      \nabla_\theta J(\theta_i)) +\frac{L\alpha^2}{2}(\|\beta_i + \nabla_\theta
      J(\theta_i)\|_2^2 + V) \\
    &= J(\theta_i) - \alpha\|\nabla_\theta J(\theta_i)\|_2^2 +
      \frac{L\alpha^2}{2}(\|\nabla_\theta J(\theta_i)\|_2^2 + V +
      \|\beta_i\|_2^2) + (L\alpha^2 - \alpha)\nabla_\theta J(\theta_i)^T\beta_i \\
    &\leq J(\theta_i) - \alpha\|\nabla_\theta J(\theta_i)\|_2^2 +
      \frac{L\alpha^2}{2}(G^2 + V + K^2H^2U^2\delta^2) + (L\alpha^2 - \alpha)\nabla_\theta J(\theta_i)^T\beta_i \\
    &\leq J(\theta_i) - \alpha\|\nabla_\theta J(\theta_i)\|_2^2 +
      \frac{L\alpha^2}{2}(G^2 + V + K^2H^2U^2\delta^2) + (L\alpha^2 + \alpha)\|\nabla_\theta
      J(\theta_i)\|\|\beta_i\| \\
    &\leq J(\theta_i) - \alpha\|\nabla_\theta J(\theta_i)\|_2^2 +
      \frac{L\alpha^2}{2}(G^2 + V + K^2H^2U^2\delta^2) + (L\alpha^2 + \alpha)GKUH\delta
  \end{align*}
  Rearranging terms and summing over timestep $1$ to $T$, we get
  \begin{align*}
    &\alpha\sum_{i=1}^T \|\nabla_\theta J(\theta_i)\|_2^2 \leq J(\theta_0) -
      \mathbb{E}_T[J(\theta_T)] + \frac{LT\alpha^2}{2}(G^2 + V +
      K^2H^2U^2\delta^2) + (L\alpha^2 + \alpha)GKUHT\delta \\
    &\leq \Delta_0 + \frac{LT\alpha^2}{2}(G^2 + V +
      K^2H^2U^2\delta^2) + (L\alpha^2 + \alpha)GKUHT\delta \\
    &\sum_{i=1}^T \|\nabla_\theta J(\theta_i)\|_2^2 \leq
      \frac{\Delta_0}{\alpha} + \frac{LT\alpha}{2}(G^2 + V +
      K^2H^2U^2\delta^2) + (L\alpha + 1)GKUHT\delta \\
    &\leq \frac{\Delta_0}{\alpha} + \frac{LT\alpha}{2}(G^2 +
      K^2H^2U^2\delta^2 + 2GKUH\delta)+ GKUHT\delta + \frac{LT\alpha}{2}V \\
    &\leq \frac{\Delta_0}{\alpha} + \frac{LT\alpha}{2}(G+KHU\delta)^2
    + GKUHT\delta +
      \frac{LT\alpha K^2p^2H^2}{\delta^2}((\Qbound + W\delta)^2 + \sigma^2) \\
      &\leq  \frac{\Delta_0}{\alpha} + LT\alpha(G^2 + K^2H^2U^2\delta^2) + GKUHT\delta + 2\frac{LT\alpha K^2p^2H^2}{\delta^2}(\Qbound^2 + W^2\delta^2 +\sigma^2)
  \end{align*}
  Using $\Delta_0 \leq \Qbound$ and optimizing over $\alpha$ and $\delta$, we get $\alpha = \mathcal{O}(\Qbound^{\frac{3}{4}}T^{-\frac{3}{4}}H^{-1}p^{-\frac{1}{2}}(\Qbound^2 + \sigma^2)^{-\frac{1}{4}})$ and $\delta = \mathcal{O}(T^{-\frac{1}{4}}p^{\frac{1}{2}}(\Qbound^2 + \sigma^2)^{\frac{1}{4}})$. This gives us
  \begin{equation}
      \frac{1}{T}\sum_{i=1}^T \|\nabla_\theta J(\theta_i)\|_2^2 \leq \mathcal{O}(T^{-\frac{1}{4}}Hp^{\frac{1}{2}}(\Qbound^3 + \sigma^2\Qbound)^{\frac{1}{4}})
  \end{equation}

\end{proof}

\iffalse
\subsection{Lower Bound Construction and Proof}

\begin{corollary}
to do
\end{corollary}
\begin{proof}
Since the convex decision set $\mathrm{W} = \{w: \|w\|_{2}\leq 1\}$ is bounded and compact, we can pick two points $w$ and $w'$ from $\mathrm{W}$ such that $\| w - w'\|_2 = 1$. Since $\|w - w'\|_2 = \sup_{l:\|l\|_2\leq 1}\langle l, w - w' \rangle$, and the set $\{l: \|l\|_2 \leq \}$ is compact, there exists $l$ such that $l^{\top}(w- w') = 1$ and $\|l\|_2 = 1$.

At any round $t$, we construct a linear loss function as follows. We sample a Rademacher variable $Z_t \in \{-1,1\}$ uniformly randomly, and the loss function $\ell_t(w) = (c Z_t l)^{\top} w$ with $c\in\mathbb{R}^+$.  Note that for any $t$, $\mathbb{E}_t[\ell_t(w)] = \mathbf{0}^{\top} w \triangleq \ell(w)$, as $\mathbb{E}_t [c Z_t] = \mathbf{0}$. Also, $\nabla_{w} \ell_t(w) = c Z_t l$ and $\mathbb{E}_{t}[\nabla_{w}\ell_t(w)] = \mathbf{0}$, which is equal to $\nabla_{w} \ell(w)$. Hence, running Online gradient descent on the sequence of loss function $\{\ell_t(w)\}$ is equivalent to running SGD with stochastic gradient $\nabla_{w} \ell_t(w)$ on the loss function $\ell(w)$.

Define regret as:
\begin{align*}
    \mathrm{Regret} = \sum_{t=1}^T \ell_t(w_t) - \min_{w\in\mathrm{W}} \sum_{t=1}^{T} \ell_t(w)
\end{align*} Note that for any sequence of $\{\ell_t\}$, we have:
\begin{align*}
    w^{\star} = \arg\min_{w\in\mathrm{W}} \sum_{t=1}^T \ell_t(w) = \frac{-\sum_{t=1}^{T} c Z_t l}{\|\sum_{t=1}^{T} c Z_t l\|_2} = -c l \frac{\sum_{t=1}^T Z_t} {\lvert\sum_{t=1}^T Z_t\rvert}.
\end{align*} Hence, $\min_{w\in\mathrm{W}} \sum_{t=1}^T \ell_t(w)$ is equal to:
\begin{align*}
   \min_{w\in\mathrm{W}} \sum_{t=1}^T \ell_t(w) = -c^2 \left\lvert \sum_{t=1}^T Z_t  \right\rvert.
\end{align*}



\end{proof}
\fi

\subsection{Implementation Details}
\label{sec:impl-deta}

\subsubsection{One-step Control Experiments}
\label{sec:one-step-control-1}

\paragraph{Tuning Hyperparameters for ARS}
We tune the hyperparameters for ARS \citep{mania2018simple} in both MNIST and linear regression experiments, by choosing a candidate set of values for each hyperparameter: stepsize, number of directions sampled, number of top directions chosen and the perturbation length along each direction. The candidate hyperparameter values are shown in Table \ref{tab:hyperparam}.

\begin{table}[ht]
    \centering
    \begin{tabular}{|c|c|}
      \hline
      \textbf{Hyperparameter} & \textbf{Candidate Values}\\
    \hline
    Stepsize &  $0.001, 0.005, 0.01, 0.02, 0.03$\\
    \hline
    \# Directions &  $10, 50, 100, 200, 500$\\
    \hline
    \# Top Directions & $5, 10, 50, 100, 200$\\
    \hline
    Perturbation & $0.001, 0.005, 0.01, 0.02, 0.03$ \\
    \hline
    \end{tabular}
    \caption{Candidate hyperparameters used for tuning in ARS  experiments}
    \label{tab:hyperparam}
\end{table}

We use the hyperparameters shown in Table \ref{tab:chosen-hyperparams} chosen through this tuning for each of the experiments in this work. The hyperparameters are chosen by averaging the test squared loss across three random seeds (different from the 10 random seeds used in actual experiments) and chosing the setting that has the least mean test squared loss after 100000 samples.

\begin{table}[ht]
    \centering
    \begin{tabular}{|c|c|c|c|c|}
    \hline
    \textbf{Experiment} & \textbf{Stepsize} & \textbf{\# Dir}. & \textbf{\# Top Dir.} & \textbf{Perturbation}\\
    \hline
    MNIST     &  0.02 & 50 & 20 & 0.03\\
    \hline
    LR $d=10$     & 0.03 & 10 & 10 & 0.03 \\
    \hline
    LR $d=100$ & 0.03 & 10 & 10 & 0.02 \\
    \hline
    LR $d=1000$ & 0.03 & 200 & 200 & 0.03 \\
    \hline
    \end{tabular}
    \caption{Hyperparameters chosen for ARS in each experiment. LR is short-hand for Linear Regression.}
    \label{tab:chosen-hyperparams}
\end{table}

\begin{table}[ht]
    \centering
    \begin{tabular}{|c|c|c|}
    \hline
    \textbf{Experiment}     &  \textbf{Learning Rate} & \textbf{Batch size}\\
    \hline
    MNIST     &  0.001 & 512\\
    \hline
    LR $d=10$ & 0.08 & 512\\
    \hline
    LR $d=100$ & 0.03 & 512\\
    \hline
    LR $d=1000$ & 0.01 & 512\\
    \hline
    \end{tabular}
    \caption{Learning rate and batch size used for REINFORCE experiments. We use an ADAM \citep{kingma2014adam} optimizer for these experiments.}
    \label{tab:hyperparam-reinforce}
\end{table}

\begin{table}[ht]
    \centering
    \begin{tabular}{|c|c|c|}
    \hline
    \textbf{Experiment}     &  \textbf{Learning Rate} & \textbf{Batch size}\\
    \hline
    LR $d=10$ & 2.0 & 512\\
    \hline
    LR $d=100$ & 2.0 & 512\\
    \hline
    \end{tabular}
    \caption{Learning rate and batch size used for Natural REINFORCE experiments. Note that we decay the learning rate after each batch by $\sqrt{T}$ where $T$ is the number of batches seen.}
    \label{tab:hyperparam-nreinforce}
\end{table}

\paragraph{MNIST Experiments}
\label{sec:mnist-details}

The CNN architecture used is as shown in Figure \ref{fig:arch}\footnote{This figure is generated by adapting the code from \url{https://github.com/gwding/draw_convnet}}. The total number of parameters in this model is $d=21840$. For supervised learning, we use a cross-entropy loss on the softmax output with respect to the true label. To train this model, we use a batch size of 64 and a stochastic gradient descent (SGD) optimizer with learning rate of 0.01 and a momentum factor of 0.5. We evaluate the test accuracy of the model over all the $10000$ images in the MNIST test dataset.

\begin{figure}[H]
    \centering
    \includegraphics[width=0.9\linewidth]{figures/aistats19/conv.png}
    \caption{CNN architecture used for the MNIST experiments}
    \label{fig:arch}
\end{figure}

For REINFORCE, we use the same architecture as before. We train the model by sampling from the categorical distribution parameterized by the softmax output of the model and then computing a $\pm 1$ reward based on whether the model predicted the correct label. The loss function is the REINFORCE loss function given by,
\begin{equation}
    J(\theta) = \frac{1}{N} \sum_{i=1}^N r_i \log(\mathbb{P}(\hat y_i|x_i, \theta))
\end{equation}
where $\theta$ is the parameters of the model, $r_i$ is the reward obtained for example $i$, $\hat y_i$ is the predicted label for example $i$ and $x_i$ is the input feature vector for example $i$. The reward $r_i$ is given by $r_i = 2*\mathbb{I}[\hat y_i = y_i] - 1$, where $\mathbb{I}$ is the $0-1$ indicator function and $y_i$ is the true label for example $i$.

For ARS, we use the same architecture and reward function as before. The hyperparameters used are shown in Table \ref{tab:chosen-hyperparams} and we closely follow the algorithm outlined in \citep{mania2018simple}.

\paragraph{Linear Regression Experiments}
\label{sec:linreg-details}

We generate training and test data for the linear regression experiments as follows: we sampled a random $d+1$ dimensional vector $w$ where $d$ is the input dimensionality. We also sampled a random $d \times d$ covariance matrix $C$. The training and test dataset consists of $d+1$ vectors $x$ whose first element is always $1$ (for the bias term) and the rest of the $d$ terms are sampled from a multivariate normal distribution with mean $\mathbf{0}$ and covariance matrix $C$. The target vectors $y$ are computed as $y = w^Tx + \epsilon$ where $\epsilon$ is sampled from a univariate normal distribution with mean $0$ and standard deviation $0.001$.

We implemented both SGD and Newton Descent on the mean squared loss, for the supervised learning experiments. For SGD, we used a learning rate of $0.1$ for $d=10, 100$ and a learning rate of $0.01$ for $d=1000$, and a batch size of 64. For Newton Descent, we also used a batch size of 64. To frame it as a one-step MDP, we define a reward function $r$ which is equal to the negative of mean squared loss. Both REINFORCE and ARS use this reward function. To compute the REINFORCE loss, we take the prediction of the model $\hat{w}^Tx$, add a mean $0$ standard deviation $\beta = 0.5$ Gaussian noise to it, and compute the reward (negative mean squared loss) for the noise added prediction. The REINFORCE loss function is then given by
\begin{equation}
    J(w) = \frac{1}{N} \sum_{i=1}^N r_i \frac{- (y_i - \hat{w}^Tx_i)^2}{2\beta^2}
\end{equation}
where $r_i = -(y_i - \hat y_i)^2$, $\hat y_i$ is the noise added prediction and $\hat{w}^Tx_i$ is the prediction by the model. We use an Adam optimizer with learning rate and batch size as shown in Table \ref{tab:hyperparam-reinforce}. For the natural REINFORCE experiments, we estimate the fisher information matrix and compute the descent direction by solving the linear system of equations $Fx = g$ where $F$ is the fisher information matrix and $g$ is the REINFORCE gradient. We use SGD with a $O(1/\sqrt{T})$ learning rate, where $T$ is the number of batches seen, and batch size as shown in Table \ref{tab:hyperparam-nreinforce}.

For ARS, we closely follow the algorithm outlined in \citep{mania2018simple}.

\subsubsection{Multi-step Control Experiments}
\label{sec:multi-step-control-1}

\paragraph{Tuning Hyperparameters for ARS}
\label{sec:tuning-hyperp-ars}

We tune the hyperparameters for ARS \citep{mania2018simple} in both
mujoco and LQR experiments, similar to the one-step control
experiments. The candidate hyperparameter values are shown in Tables
\ref{tab:hyperparam-multi-ars-mujoco} and \ref{tab:hyperparam-multi-ars-lqr}. We have observed that using all the
directions in ARS is always preferable under the low horizon settings
that we explore. Hence, we do not conduct a hyperparameter search over
the number of top directions and instead keep it the same as the
number of directions.

\begin{table}[ht]
    \centering
    \begin{tabular}{|c|c|c|}
    \hline
    \textbf{Hyperparameter} & \textbf{Swimmer-v2} &
                                                    \textbf{HalfCheetah-v2}\\
    \hline
    Stepsize &  $0.03, 0.05, 0.08, 0.1, 0.15$ & $0.001, 0.003,
                                                0.005,0.008, 0.01$ \\
    \hline
    \# Directions &  $5, 10, 20$ & $5, 10, 20$\\
    \hline
    Perturbation & $0.05, 0.1, 0.15, 0.2$ & $0.01, 0.03, 0.05, 0.08$\\
    \hline
    \end{tabular}
    \caption{Candidate hyperparameters used for tuning in ARS experiments}
    \label{tab:hyperparam-multi-ars-mujoco}
\end{table}

\begin{table}[ht]
    \centering
    \begin{tabular}{|c|c|}
    \hline
    \textbf{Hyperparameter} & \textbf{LQR}\\
    \hline
    Stepsize &  $0.0001, 0.0003, 0.0005, 0.0008, 0.001, 0.003, 0.005,
               0.008, 0.01$ \\
    \hline
    \# Directions &  $10$ \\
    \hline
    Perturbation & $0.01, 0.05, 0.1$ \\
    \hline
    \end{tabular}
    \caption{Candidate hyperparameters used for tuning in ARS experiments}
    \label{tab:hyperparam-multi-ars-lqr}
  \end{table}

We use the hyperparameters shown in Tables
\ref{tab:chosen-hyperparam-multi-ars-swimmer} and \ref{tab:chosen-hyperparam-multi-ars-halfcheetah} chosen through tuning for each
of the multi-step experiments. The hyperparameters are chosen by
averaging the total reward obtained across three random seeds
(different from the 10 random seeds used in experiments presented in
Figure~\ref{fig:multistep}) and
chosing the setting that has the highest total reward after $10000$
episodes of training..

\begin{table}[ht]
    \centering
    \begin{tabular}{|c|c|c|c|}
    \hline
      \textbf{Horizon} & \textbf{Stepsize} &
                                             \textbf{\#
                                             Directions} &
                                                           \textbf{Perturbation}\\
      \hline
      $H = 1$ & 0.15  & 5  & 0.2 \\
      \hline
      $H = 2$ & 0.08  & 5 &  0.2\\
      \hline
      $H = 3$ & 0.15  & 5 &  0.2\\
      \hline
      $H = 4$ & 0.08  & 5 & 0.2 \\
      \hline
      $H = 5$ & 0.05  & 5 &  0.2\\
      \hline
      $H = 6$ & 0.08  &5  & 0.2 \\
      \hline
      $H = 7$ & 0.08  & 5 & 0.2 \\
      \hline
      $H = 8$ & 0.08  & 5 & 0.2 \\
      \hline
      $H = 9$ & 0.1  &5  & 0.2 \\
      \hline
      $H = 10$ & 0.08  &5  & 0.2 \\
      \hline
      $H = 11$ & 0.08  &5  & 0.2 \\
      \hline
      $H = 12$ & 0.1  &5  & 0.2 \\
      \hline
      $H = 13$ & 0.08  & 5 & 0.2 \\
      \hline
      $H = 14$ & 0.08  & 5 &0.2  \\
      \hline
      $H = 15$ & 0.08  & 10 & 0.2 \\
      \hline
    \end{tabular}
    \caption{Hyperparameters chosen for multi-step experiments for ARS
    in Swimmer-v2}
    \label{tab:chosen-hyperparam-multi-ars-swimmer}
\end{table}

\begin{table}[ht]
    \centering
    \begin{tabular}{|c|c|c|c|}
    \hline
      \textbf{Horizon} & \textbf{Stepsize} &
                                             \textbf{\#
                                             Directions} &
                                                           \textbf{Perturbation}\\
%0.001, 0.008, 0.008, 0.003, 0.003, 0.003, 0.008, 0.008, 0.01 ,
       %0.005, 0.008, 0.005, 0.008, 0.01 , 0.008
      \hline
      $H = 1$ & 0.001  & 20 & 0.08 \\
      \hline
      $H = 2$ & 0.008  & 5 &  0.08\\
      \hline
      $H = 3$ &  0.008  & 10 & 0.08 \\
      \hline
      $H = 4$ &  0.003  & 5 &  0.05\\
      \hline
      $H = 5$ &  0.003  & 5 &  0.05\\
      \hline
      $H = 6$ &  0.003  & 10 &  0.05\\
      \hline
      $H = 7$ &  0.008  & 20 &  0.05\\
      \hline
      $H = 8$ &  0.008  & 5 &  0.05\\
      \hline
      $H = 9$ &  0.01  & 20 &  0.03\\
      \hline
      $H = 10$ &   0.005 & 10 &  0.03\\
      \hline
      $H = 11$ &  0.008  & 20 &  0.03\\
      \hline
      $H = 12$ &  0.005  & 5 &  0.05\\
      \hline
      $H = 13$ &  0.008  & 20 &  0.03\\
      \hline
      $H = 14$ &  0.01  & 10 &  0.03\\
      \hline
      $H = 15$ &  0.008  & 20 &  0.03\\
      \hline
    \end{tabular}
    \caption{Hyperparameters chosen for multi-step experiments for ARS
    in HalfCheetah-v2}
    \label{tab:chosen-hyperparam-multi-ars-halfcheetah}
\end{table}


\paragraph{Tuning Hyperparameters for ExAct}
\label{sec:tuning-hyperp-exact}

We tune the hyperparameters for ExAct (Algorithm
\ref{alg:random_search_action}) in both mujoco and LQR experiments,
similar to ARS. The candidate hyperparameter values are shown in
Tables \ref{tab:hyperparam-multi-exact-mujoco} and
\ref{tab:hyperparam-multi-exact-lqr}. Similar to ARS, we do not
conduct a hyperparameter search over the number of top directions and
instead keep it the same as the number of directions.

\begin{table}[H]
    \centering
    \begin{tabular}{|c|c|c|}
    \hline
    \textbf{Hyperparameter} & \textbf{Swimmer-v2} &
                                                    \textbf{HalfCheetah-v2}\\
    \hline
    Stepsize &  $0.005, 0.008, 0.01, 0.015, 0.02, 0.025, 0.03$ & $0.0001, 0.0003,
                                                0.0005,0.0008, 0.001,
                                                                 0.002,
                                                                 0.003$ \\
    \hline
    \# Directions &  $5, 10, 20$ & $5, 10, 20$\\
    \hline
    Perturbation & $0.15, 0.2, 0.3, 0.5$ &  $0.15, 0.2, 0.3, 0.5$\\
    \hline
    \end{tabular}
    \caption{Candidate hyperparameters used for tuning in ExAct experiments}
    \label{tab:hyperparam-multi-exact-mujoco}
\end{table}

\begin{table}[ht]
  \centering
  \begin{tabular}{|c|c|}
    \hline
    \textbf{Hyperparameter} & \textbf{LQR}\\
    \hline
    Stepsize &  $0.0001, 0.0003, 0.0005, 0.0008, 0.001, 0.003, 0.005,
               0.008, 0.01$ \\
    \hline
    \# Directions &  $10$ \\
    \hline
    Perturbation & $0.01, 0.05, 0.1$ \\
    \hline
  \end{tabular}
  \caption{Candidate hyperparameters used for tuning in ExAct experiments}
  \label{tab:hyperparam-multi-exact-lqr}
\end{table}

We use the hyperparameters shown in Tables
\ref{tab:chosen-hyperparam-multi-exact-swimmer} and \ref{tab:chosen-hyperparam-multi-exact-halfcheetah} chosen through tuning for
each of the multi-step experiments, similar to ARS.

\begin{table}[ht]
    \centering
    \begin{tabular}{|c|c|c|c|}
    \hline
      \textbf{Horizon} & \textbf{Stepsize} &
                                             \textbf{\#
                                             Directions} &
                                                           \textbf{Perturbation}\\
      \hline
      $H = 1$ & 0.02  &5  & 0.2 \\
      \hline
      $H = 2$ & 0.02  & 5 & 0.2 \\
      \hline
      $H = 3$ & 0.015  & 10 & 0.2 \\
      \hline
      $H = 4$ & 0.015  & 10 & 0.2 \\
      \hline
      $H = 5$ & 0.01  & 10 & 0.2 \\
      \hline
      $H = 6$ & 0.015  & 10 & 0.2 \\
      \hline
      $H = 7$ & 0.01  & 20 & 0.2 \\
      \hline
      $H = 8$ & 0.015  & 20 & 0.2 \\
      \hline
      $H = 9$ & 0.02  & 20 & 0.2 \\
      \hline
      $H = 10$ & 0.008  & 5 & 0.2 \\
      \hline
      $H = 11$ & 0.02  & 5 & 0.15 \\
      \hline
      $H = 12$ & 0.02  & 20 & 0.2 \\
      \hline
      $H = 13$ & 0.015  & 5 & 0.15 \\
      \hline
      $H = 14$ & 0.02  & 10 &0.15  \\
      \hline
      $H = 15$ & 0.01  & 5 & 0.1 \\
      \hline
    \end{tabular}
    \caption{Hyperparameters chosen for multi-step experiments for ExAct
    in Swimmer-v2}
    \label{tab:chosen-hyperparam-multi-exact-swimmer}
\end{table}

\begin{table}[ht]
    \centering
    \begin{tabular}{|c|c|c|c|}
    \hline
      \textbf{Horizon} & \textbf{Stepsize} &
                                             \textbf{\#
                                             Directions} &
                                                           \textbf{Perturbation}\\

      \hline
      $H = 1$ &0.0001   &20  &  0.2\\
      \hline
      $H = 2$ &  0.001 & 5 &  0.2\\
      \hline
      $H = 3$ &  0.001  & 5 & 0.2\\
      \hline
      $H = 4$ &  0.001  & 5 & 0.2 \\
      \hline
      $H = 5$ &  0.001  &10  & 0.2 \\
      \hline
      $H = 6$ &  0.001  & 5 & 0.2 \\
      \hline
      $H = 7$ &  0.001  &10  & 0.2 \\
      \hline
      $H = 8$ &  0.001  & 5 & 0.2 \\
      \hline
      $H = 9$ &  0.001  & 5 & 0.2 \\
      \hline
      $H = 10$ &  0.001  & 5 & 0.2 \\
      \hline
      $H = 11$ & 0.0008   & 5 & 0.15 \\
      \hline
      $H = 12$ &  0.001  & 5 & 0.2\\
      \hline
      $H = 13$ &  0.001  & 10 & 0.2 \\
      \hline
      $H = 14$ &  0.001  & 5 & 0.2\\
      \hline
      $H = 15$ &  0.0008  & 10 & 0.2 \\
      \hline
    \end{tabular}
    \caption{Hyperparameters chosen for multi-step experiments for ExAct
    in HalfCheetah-v2}
    \label{tab:chosen-hyperparam-multi-exact-halfcheetah}
\end{table}


\paragraph{Mujoco Experiments}
\label{sec:mujoco-experiments}

For all the mujoco experiments, both ARS and ExAct use a linear
policy with the same number of parameters as the dimensionality of the
state space. The hyperparameters for both algorithms are chosen as
described above. Each algorithm is run on both
environments (Swimmer-v2
and HalfCheetah-v2) for $10000$ episodes of training across $10$
random seeds (different from the ones used for tuning). This is
repeated for each horizon value $H \in \{1, 2, \cdots, 15\}$. In each
experiment, we record the mean evaluation return obtained after
training and plot the results in Figure~\ref{fig:multistep}. For more details on the environments used, we
refer the reader to \citep{brockman2016openai}.

\paragraph{LQR Experiments}
\label{sec:lqr-experiments}

In the LQR experiments, we constructed a linear dynamical system
$x_{t+1} = Ax_t + Bu_t + \xi_t$ where $x_t \in \mathbb{R}^{100}$, $A \in \mathbb{R}^{100\times
  100}$, $B \in \mathbb{R}^{100}$, $u_t \in
\mathbb{R}$ and the noise $\xi_t \sim \mathcal{N}(0_{100}, cI_{100
  \times 100})$ with a small constant $c \in \mathbb{R}^+$. We
explicitly make sure that the maximum eigenvalue of $A$ is less than 1
to avoid instability. We fix a quadratic cost function $c(x, u) =
x^TQx + uRu$, where $Q = 10^{-3}I_{100 \times 100}$ and $R = 1$. The
hyperparameters chosen for both algorithms are chosen as described
above.

For each algorithm, we run it for noise covariance values $c \in
\{10^{-4}, 5\times 10^{-4}, 10^{-3}, 5\times 10^{-3}, 10^{-2}, 5\times 10^{-2},
10^{-1}, 5\times 10^{-1}\}$
until we reach a stationary point where $\|\nabla_\theta
J(\theta)\|_2^2 \leq 0.05$. The number of interactions with the
environment allowed is capped at $10^6$ steps for each run. This is
repeated across $10$ random seeds (different from the ones used for
tuning). The number of interactions needed to reach the stationary
point as the noise covariance is increased is recorded and shown in
Figure~\ref{fig:multistep}.
\clearpage
\newpage
\section{Appendix for Chapter~\ref{CHA:CMAX}}
\label{sec:append-chapt-cmax}

\subsection{A Closer Look at the Assumption}
\label{sec:closer-look-at}

Assumption~\ref{assumption:core} requires that there exists at least
one path from the \textit{current state} to a goal state that does not
contain any transition that is known to be incorrect. In other words, it should
hold for every time step $t$ before the robot reaches the goal. Hence,
it is an assumption on both the quality of the approximate model and
the execution trace (the states visted during execution) of
\textsc{Cmax}. This makes the assumption hard to verify prior to
execution as it is
dependent on the operation of the algorithm under true dynamics.

However, we can relax the assumption in small state spaces by using the
\textit{model optimism} assumption from
\citet{DBLP:conf/aaai/Jiang18}. Specifically, an optimistic model is an
approximate model $\hat{M}$ whose optimal cost-to-go under its dynamics
$\hat{f}$ at each state $s \in \statespace$ underestimates the optimal
cost-to-go under true dynamics $f$. Notice that in small state spaces,
we can afford to do full state space planning (using value iteration,
for example) at each time step $t$, thereby obtaining the best action
under the approximate model's optimal cost-to-go. Thus, if the initial
approximate model $\hat{M}$ in \textsc{Cmax} is optimistic and we
perform full state space planning at each time step $t$, then we are
guaranteed to reach the goal. In other words, optimistic intial model
$\hat{M}$ is \textit{sufficient} for completeness if we perform full
state space planning at each time step.

Unfortunately, performing full state space planning at each time step
is computationally very expensive in large state spaces (or
intractable in continuous state spaces.) Hence, we need to resort to
limited-expansion planning or sample-based
planning~\cite{DBLP:journals/ml/KearnsMN02} which can result in
suboptimal actions. In these cases, model optimism is not sufficient
anymore, and we require assumptions such as
Assumption~\ref{assumption:core} and \ref{assumption:core-large} that
rely on the execution trace to guarantee completeness of \textsc{Cmax}.

\subsection{4D Planar Pushing Experiment Details}
\label{sec:4d-planar-pushing}

In this experiment, the task is for a robotic gripper to push a cube
from a start location to a goal location in the presence of
static obstacles without any resets, as shown in
Figure~\ref{fig:search} (right). This can be represented as a
planning problem in 4D continuous state space $\statespace$ with any state represented as
the tuple $s = (g_x,
g_y, o_x, o_y)$ where $(g_x, g_y)$ are the xy-coordinates of the
gripper and $(o_x, o_y)$ are the xy-coordinates of the object. The
model $\hat{M}$ used for planning \textit{does not} have the static obstacles and the
robot can only discover the state-action pairs that are affected due
to the obstacles through real world executions. The
action space $\actionspace$ is a discrete set of 4 actions that move
the gripper end-effector in the 4 cardinal directions by a fixed
offset using an IK-based controller. The cost of each transition is
$1$ when the object is not at the goal location, and $0$
otherwise.

For all the approaches (except Q-learning), we use
the following neural network architecture for cost-to-go
approximation: a feedforward network with 3 hidden layers each of $64$
units, the network takes as input a $15$D feature representation of the 4D
state $s = (o_x, o_y, g_x, g_y)$ that is constructed as follows:
\begin{itemize}
\item Relative position of the object w.r.t gripper $\frac{\mathbf{o}
    - \mathbf{g}}{\|\mathbf{o} - \mathbf{g}\|_2}$, where $\mathbf{o} =
  (o_x, o_y)$ is the $2$D object position and $\mathbf{g} = (g_x,
  g_y)$ is the $2$D gripper position
\item Distance between position of the object and gripper
  $\|\mathbf{o} - \mathbf{g}\|_2$
\item Relative position of the object w.r.t. goal $\frac{\mathbf{o} -
    \mathbf{t}}{\|\mathbf{o} - \mathbf{t}\|_2}$ where $\mathbf{t} =
  (t_x, t_y)$ is the $2$D goal location
\item Distance between position of the object and goal location
  $\|\mathbf{o} - \mathbf{t}\|_2$
\item Relative position of the gripper w.r.t goal $\frac{\mathbf{g} -
    \mathbf{t}}{\|\mathbf{g} - \mathbf{t}\|_2}$
\item Distance between position of the gripper and goal location
  $\|\mathbf{g} - \mathbf{t}\|_2$
\item Relative position of the object w.r.t center of the table
  $\frac{\mathbf{o} - \mathbf{c}}{\|\mathbf{o} - \mathbf{c}\|_2}$
\item Distance between position of the object and center of the table
  ${\|\mathbf{o} - \mathbf{c}\|_2}$
\item Relative position of the gripper w.r.t center of the table
  $\frac{\mathbf{g} - \mathbf{c}}{\|\mathbf{g} - \mathbf{c}\|_2}$
\item Distance between position of the gripper and center of the table
  ${\|\mathbf{g} - \mathbf{c}\|_2}$
\end{itemize}

The output of the network is a single scalar value representing the
cost-to-go of the input state. We use ReLU activations after each
layer except the last layer. Instead of learning the cost-to-go from
scratch, we start with an initial cost-to-go estimate that is
hardcoded and the neural network function approximator is used to
learn a residual on top of it. The hardcoded initial cost-to-go
estimate is obtained as follows:
\begin{itemize}
\item For the given object position, construct a target position for
  the gripper to go to as follows:
  \begin{itemize}
  \item Get the angle of the vector pointing from the object to the
    goal location: $\theta = \tan^{-1}(\frac{t_x - o_x}{t_y - o_y})$
  \item The target position for gripper is then given by $\mathbf{gt}
    = (o_x - \frac{\sin(\theta)w}{2}, o_y - \frac{\cos(\theta)w}{2})$
    where $w$ is the width of the object
  \end{itemize}
\item We compute the manhattan distance from the gripper to its target
  position $M(\mathbf{g}, \mathbf{gt})$, and from the object to the
  goal location $M(\mathbf{o}, \mathbf{t})$
\item The hardcoded heuristic is obtained as $\hat{V}(s) =
  \frac{M(\mathbf{g}, \mathbf{gt}) + M(\mathbf{o}, \mathbf{t})}{d}$,
  where $d$ is the fixed offset distance the gripper moves for each action
\end{itemize}
The residual cost-to-go function approximator is initialized in such a
way that it outputs $0$ initially for all $s \in \statespace$. We use
a similar residual Q-value function approximator for Q-learning with
the same architecture but that takes as input the above feature
representation and outputs a vector in $\reals^{|\actionspace|}$,
where each element corresponds to the Q-value for that action in the
input state. We also use hardcoded initial Q-values that are
constructed in a similar fashion $\hat{Q}(s, a) = c(s, a) +
\hat{V}(\hat{f}(s, a))$. To ensure a fair comparison across all
baselines, we use the same neural network function approximator for
cost-to-go, and start with the same initial cost-to-go
estimates.

For the model learning baseline that uses Neural network function
approximator, we use a feedforward neural network with $2$ hidden
layers each of $32$ units, the network takes as input the 4D state $s$
and a one-hot encoding of the discrete action $a$ and outputs a 4D
residual vector. The residual vector is added to the next state
predicted by the model $\hat{f}(s, a)$ to get the learned next
state. The loss function used to train the residual is mean
squared loss.

For the model learning baseline that uses KNN function approximator,
we use a radius of $0.02$, and average the next state residual vector observed
for any state within this radius to obtain the prediction for a new
state residual vector. In the same way as above, this residual vector
is added to the next state predicted by the model $\hat{f}(s, a)$ to
obtain the learned next state.

For all the neural network function approximators, we use an Adam
optimizer with learning rate of $0.001$, and an L2 regularization
constant of $0.01$. We use a batch size of $64$ for training all the
neural network function approximators. For Q-learning, we use an
random exploration probability of $\epsilon = 0.1$ and change the
target network by a polyak coefficient of $0.9$.

For all the approaches, we use a
limited expansion search planner with $K = 5$ expansions, $N = 5$
planning updates, batch size $B = 64$, and an Adam optimizer \cite{DBLP:journals/corr/KingmaB14} with
learning rate $\eta = 0.001$.

In training the cost-to-go function approximation, we use the
hindsight experience replay trick with a probability of $0.8$ for
sampling any future state in the trajectory as the desired goal. This
helps in keeping the function approximation stable and also helps in
generalization.

\subsection{3D Pick-and-Place Experiment Details}
\label{sec:3d-pick-place}

The task of this physical robot experiment (Figure~\ref{fig:real-3d})
is to pick and place a heavy object using a PR2 arm from a start pick
location to a goal place
location while avoiding an obstacle. This can be represented as a
planning problem in 3D discrete state space $\statespace$ where
each state corresponds to the 3D location of the end-effector. In our
experiment, we discretize each dimension into $20$ bins and plan in
the resulting discrete state space of size $20^3$. Since it is a
relatively small state space, we use exact planning updates without
any function approximation following
Algorithm~\ref{alg:small-state-spaces} with $K=3$ expansions. The action space is a
discrete set of $6$ actions corresponding to a fixed offset movement
in positive or negative direction along each dimension. We use a
RRT-based motion planner \cite{DBLP:journals/ijrr/LaValleK01} to plan the path of the
arm between states, while avoiding collision with the obstacle. The
model $\hat{M}$ used by planning \textit{does not} model the object as
heavy and hence, does not capture the dynamics of the arm correctly when it
holds the heavy object. The cost of each transition is $1$ if
object is not at the goal place location, otherwise it is $0$.

\subsection{7D Arm Planning Experiment Details}
\label{sec:7d-arm-planning}

The task of this physical robot experiment (Figure~\ref{fig:real-7d}) is
to move the PR2 arm with a
non-operational joint from a start configuration so that the
end-effector reaches a goal location, specified as a 3D
region. We represent this as a planning problem in 7D
discrete statespace $\statespace$ where each dimension corresponds to
a joint of the arm bounded by its joint limits. Each dimension is
discretized into $10$ bins resulting in a large state space of
size $10^7$. The action space
$\actionspace$ is a discrete set of size
$14$ corresponding to moving each joint by a fixed offset in the
positive or negative direction. We use an IK-based controller to
navigate between discrete states. The model $\hat{M}$ used for
planning \textit{does not} know that a joint is non-operational and
assumes that the arm can attain any configuration within the joint
limits. In the real world, if the robot tries to move the
non-operational joint, the arm does not move. Thus, the robot realizes
unreachable states only through real world executions.

\clearpage
\newpage
\section{Appendix for Chapter~\ref{CHA:CMAXPP}}
\label{sec:append-chapt-cmaxpp}

\subsection{Sensitivity Experiments}
\label{sec:sens-exper}

In this section, we present the results of our sensitivity experiments
examining the performance of \acmaxpp{} with the choice of the
sequence $\{\alpha_i\}$. We compare the performance of different
choices of the sequence $\{\alpha_i\}$ on the $3$D mobile robot
navigation task. For each run, we average the results across $5$
instances with randomly placed ice patches and present the mean and
standard errors. To keep the figures concise, we plot the cumulative
number of steps taken to reach the goal from the start of the first
lap to the current lap across all laps. In all our runs, \acmaxpp{}
successfully completes all $200$ laps and hence, we do not report the
number of successful instances in our results.

We choose $4$ schedules for the sequence $\{\alpha_i\}$:
\begin{enumerate}
\item \textbf{Exponential Schedule}: In this schedule, we vary
  $\beta_{i+1} = \rho\beta_i$ where $\rho < 1$ is a constant that is
  tuned and $\alpha_i = 1 + \beta_i$. Observe that as $i\rightarrow
  \infty$, $\alpha_i \rightarrow 1$ and that the sequence
  $\{\alpha_i\}$ is a decreasing sequence.

  We vary both the initial $\beta_1$ chosen and the constant $\rho$ in
  our experiments. For $\beta_1$ we choose among values $[10, 100,
  1000]$ and $\rho$ is chosen among $[0.5, 0.7, 0.9]$. The results are
  shown in Figure~\ref{fig:exp}.
  \begin{figure}
  \centering
  \includegraphics[width=0.5\linewidth]{figures/cmaxpp/alpha_exp.pdf}
  \caption{Sensitivity experiments with an exponential schedule}
  \label{fig:exp}
\end{figure}

All choices have almost the same performance with $\beta_1 = 1000$ and
$\rho = 0.9$ having the best performance initially but has slightly
worse performance in the last several laps. The choice of $\beta_1 =
100$ and $\rho = 0.9$  seems to be a good choice with great performance in both
initial and final laps.

\item \textbf{Linear Schedule}: In this schedule, we vary $\beta_{i+1}
  = \beta_i - \eta$ where $\alpha_i = 1 + \beta_i$ and $\eta > 0$ is a
  constant that is determined
  so that $\beta_{200} = 0$, i.e. $\alpha_{200} = 1$. Hence, we have
  $\eta = \frac{\beta_1}{200}$.

  We vary the initial $\beta_1$ and choose among values $[10, 100,
  200]$. The results are shown in Figure~\ref{fig:linear}.
  \begin{figure}
  \centering
  \includegraphics[width=0.5\linewidth]{figures/cmaxpp/alpha_linear.pdf}
  \caption{Sensitivity experiments with a linear schedule}
  \label{fig:linear}
\end{figure}

All three choices have the same performance except in the last few
laps where $\beta_1 = 10$ degrades while the other two choices perform well.
\item \textbf{Time Decay Schedule}: In this schedule, we vary
  $\beta_{i+1} = \frac{\beta_1}{i+1}$ where $\alpha_i = 1 +
  \beta_i$. In other words, we decay $\beta$ at the rate of
  $\frac{1}{i}$ where $i$ is the lap number. Again, observe that as $i
  \rightarrow \infty$, we have $\alpha_i \rightarrow 1$.

  We vary the initial $\beta_1$ and choose among values $[10, 100,
  1000]$. The results are shown in Figure~\ref{fig:time}.
  \begin{figure}
  \centering
  \includegraphics[width=0.5\linewidth]{figures/cmaxpp/alpha_time.pdf}
  \caption{Sensitivity experiments with a time decay schedule}
  \label{fig:time}
\end{figure}

The choices of $\beta_1 = 100$  and $\beta_1 = 1000$ have the best
(and similar) performance while $\beta_1 = 10$ has a poor performance
as it quickly switches to \cmaxpp{} in the early laps and wastes
executions learning accurate $Q$-values.
\item \textbf{Step Schedule}: In this schedule, we vary $\beta$ as a
  step function with $\beta_{i+1} = \beta_i - \delta$ if $i$ is a
  multiple of $\xi$ where $\xi$ is the step frequency, $\alpha_i = 1 +
  \beta_i$ and $\delta$ is a constant that is determined so that
  $\beta_{200} = 0$, i.e. $\alpha_{200} = 1$. Hence, we have $\delta =
  \frac{\beta_1\xi}{200}$.

  We vary both the initial $\beta_1$ and the step frequency $\xi$. For
  $\beta_1$ we choose among values $[10, 100, 200]$ and for $\xi$ we
  choose among $[5, 10, 20]$. The results are shown in
  Figure~\ref{fig:step}.
  \begin{figure}
  \centering
  \includegraphics[width=0.5\linewidth]{figures/cmaxpp/alpha_step.pdf}
  \caption{Sensitivity experiments with a step schedule}
  \label{fig:step}
\end{figure}

All choices have the same performance and \acmaxpp{} seems to be
robust to the choice of step size frequency.
\end{enumerate}

For our final comparison, we will pick the best performing choice
among all the schedules and compare performance among these selected
choices. The results are shown in Figure~\ref{fig:best}.

\begin{figure}
  \centering
  \includegraphics[width=0.5\linewidth]{figures/cmaxpp/alpha_best.pdf}
  \caption{Sensitivity experiments with best choices among all
    schedules}
  \label{fig:best}
\end{figure}

We can observe that all schedules have the same performance except the
exponential schedule which has worse performance. This can be
attributed to the rapid decrease in the value of $\beta$ compared to
other schedules and thus, around lap $50$ \acmaxpp{} switches to
\cmaxpp{} resulting in a large number of executions wasted to learn
accurate $Q$-value estimates. This does not happen for other schedules
as they decrease $\beta$ gradually and thus, spreading out the
executions used to learn accurate $Q$-value estimates across several
laps and not performing poorly in any single lap.

\subsection{Experiment Details}
\label{sec:experiment-details}

All experiments were implemented using Python 3.6 and run on a
$3.1$GHz Intel Core i$5$ machine. We use
PyTorch~\cite{NEURIPS2019_9015} to train neural network function
approximators in our $7$D experiments, and use Box2D~\cite{catto2007box2d} for our 3D mobile
robot simulation (similar to OpenAI Gym~\cite{brockman2016openai}
\texttt{car\_racing} environment) and use
PyBullet~\cite{coumans2013bullet} for our $7$D PR2 experiments.

\subsubsection{3D Mobile Robot Navigation with Icy Patches}
\label{sec:3d-mobile-robot}

\begin{figure}[t]
  \centering
  \includegraphics[width=0.5\linewidth]{figures/cmaxpp/race_track_full_pic.png}
  \caption{$3$D Mobile Robot experiment example track}
  \label{fig:track}
\end{figure}

An example track used in the $3$D experiment is shown in
Figure~\ref{fig:track}. We generate $66$ motion primitives offline
using the following procedure: (a) We first define the primitive
action set for the robot by discretizing the steering angle into 3
cells, one corresponding to zero and the other two corresponding to
$+0.6$ and $-0.6$ radians. We also discretize the speed of the robot
to 2 cells corresponding to $+2$m/s and $-2$m/s, (b) We then
discretize the state space into a $100\times 100$ grid in $XY$ space
and $16$ cells in $\theta$ dimension. Thus, we have a $100 \times 100
\times 16$ grid in $XY\theta$ space., (c) We then initialize the robot
at $(0, 0)$ $xy$ location with different headings chosen among $[0,
\cdots, 15]$ and roll out all possible sequences of primitive actions
for all possible motion primitive lengths from $1$ to $15$ time steps,
(d) We filter out all motion primitives whose end point is very close
to a cell center in the $XY\theta$ grid. During execution, we use a
pure pursuit controller to track the motion primitive so that the robot
always starts and ends on a cell center. During planning, we simply
use the discrete offsets stored in the motion primitive to compute the
next state (and thus, the model dynamics are pre-computed offline
during motion primitive generation.)

The cost function used is as follows: for any motion primitive $a$ and
state $s$,
the cost of executing $a$ from $s$ is given by $c(s, a) = \sum_{s'}
c'(s')$ where $c'$ is a pre-defined cost map over the $100 \times 100
\times 16$ grid and $s'$ is all the intermediate states (including the
final state) that the robot
goes through while executing the motion primitive $a$  from $s$. The
pre-defined cost map is defined as follows: $c'(s) = 1$ if state $s$
lies on the track (i.e. $xy$ location corresponding to $s$ lies on the
track) and $c'(s) = 100$ otherwise (i.e. all $xy$ locations
corresponding to grass or wall has a cost of $100$). This encourages
the planner to come up with a path that lies completely on the track.

We define two checkpoints on the opposite ends of the track (shown as
blue squares in Figure~\ref{fig:track}.) The goal of the robot is to
reach the next checkpoint incurring least cost while staying on the
track. Note that this
requires the robot to complete laps around the track as quickly as
possible. Since the state space is small, we maintain value estimates
$V, Q, \tilde{V}$ using tables and update the appropriate table entry
for each value update. The tables are initialized with value estimates
obtained by planning in the model $\Mhat$ using a planner with $K=100$
expansions until the robot can efficiently complete the laps using the
optimal paths. However, this does not mean that the initial value
estimates are the optimal values for $\Mhat$ dynamics since the
planner looks ahead and can achieve optimal paths with underestimated
value functions. Nevertheless, these estimates are highly informative.

\subsubsection{7D Pick-and-Place with a Heavy Object}
\label{sec:7d-pick-place}

\begin{figure}[t]
  \centering
  \includegraphics[width=0.5\linewidth]{figures/cmaxpp/intro_grasp_new.png}
  \caption{$7$D Pick-and-Place Experiment}
  \label{fig:pr2}
\end{figure}

For our $7$D experiments, we make use of Bullet Physics Engine through
the pyBullet interface. For motion planning and other simulation
capabilities we make use of \texttt{ss-pybullet}
library~\cite{sspybullet}. The task is shown in
Figure~\ref{fig:pr2}. The goal is for the robot to pick the heavy
object from its start pose and place it at its goal pose while
avoiding the obstacle, without any resets. Since the object is heavy,
the robot fails to lift the object in certain configurations where it
cannot generate the required torque to lift the object. Thus, the
robot while lifting the object might fail to reach the goal waypoint
and onky reach an intermediate waypoint resulting in discrepancies
between modeled and true dynamics.


This is represented as a planning problem in $7$D statespace. The
first $6$ dimensions correspond to the $6$DOF pose of the object (or
gripper,) and the last dimension corresponds to the redundant DOF in
the arm (in our case, it is the upper arm roll joint.) Given a $7$D
configuration, we use IKFast library~\cite{diankov_thesis} to compute
the corresponding $7$D joint angle configuration. The action space
consists of $14$ motion primitives that move the arm by a fixed offset
in each of the $7$ dimensions in positive and negative directions. The
discrepancies in this experiment are only in the $Z$ dimension
corresponding to lifting the object. For planning, we simply use a
kinematic model of the arm and assume that the object being lifted is
extremely light. Thus, we do not need to explicitly consider dynamics
during planning. However, during execution we take the dynamics into
account by executing the motion primitives in the simulator. The cost
of any transition is $1$ if the object is not at goal pose, $0$ if the
object is at goal pose. We start the next repetition only if the robot
reached the goal pose in the previous repetition.


The $7$D state space is discretized into $10$ cells in each dimension
resulting in $10^7$ states. Since the state space is large we use
neural network function approximators to maintain the value functions
$V, Q, \tilde{V}$. For the state value functions $V, \tilde{V}$ we use
the following neural network approximator: a feedforward network with
$3$ hidden layers consisting of $64$ units each, we use ReLU
activations after each layer except the last layer, the network takes
as input a $34$D feature representation of the $7$D state computed as
follows:

\begin{itemize}
	\item For any discrete state $s$, we compute a continuous
          $10$D representation $r(s)$ that is used to construct the
          features

	\begin{itemize}
		\item The discrete state is represented as $(xd, yd,
                  zd, rd, pd, yd, rjointd)$ where $(xd, yd, zd)$
                  represents the $3$D discrete location of the object
                  (or gripper,) $(rd, pd, yd)$ represents the discrete
                  roll, pitch, yaw of the object (or gripper,) and
                  $rjointd$ represents the discrete redundant joint
                  angle

		\item We convert $(xd, yd, zd)$ to a continuous
                  representation by simply dividing by the grid size
                  in those dimensions, i.e. $(xc, yc, zc) = (xd/10,
                  yd/10, zd/10)$

		\item We do a similar construction for $rjointc$,
                  i.e. $rjointc = rjointd/10$

		\item However, note that $rd, pd, yd$ are angular
                  dimensions and simply dividing by grid size would
                  not encode the wrap around nature that is inherent
                  in angular dimensions (we did not have this problem
                  for $rjointd$ as the redundant joint angle has lower
                  and upper limits, and is always recorded as a value
                  between those limits.) To account for this, we use a
                  sine-cosine representation defined as $(rc1, rc2,
                  pc1, pc2. yc1, yc2) = (sin(rc), cos(rc), sin(pc),
                  cos(pc), sin(yc), cos(yc))$ where $rc, pc, yc$ are
                  the roll, pitch, yaw angles corresponding to the
                  cell centers of the grid cells $rd, pd, yd$.

		\item Thus, the final $10$D representation of state
                  $s$ is given by $r(s) = (xc, yc, zc, rc1, rc2, pc1,
                  pc2, yc1, yc2, rjointc)$

		\item We also define a truncated $9$D representation
                  $r'(s) = (xc, yc, zc, rc1, rc2, pc1, pc2, yc1, yc2)$
                  and a $3$D representation $r''(s) = (xc, yc, zc)$

	\end{itemize}
	\item The first feature is the $9$D relative position of the $6$D goal pose w.r.t the object $f1 = r'(g) - r'(s)$
	\item The second feature is the $10$D relative position of the object w.r.t the gripper home state $h$, $f2 = r(s) - r(h)$
	\item The third feature is the $9$D relative position of the goal w.r.t the gripper home state $h$, $f3 = r'(g) - r'(h)$
	\item The fourth feature is the $3$D relative position of the obstacle left top corner $o1$ w.r.t the object, $f4 = r''(o1) - r''(s)$
	\item The fifth and final feature is the $3$D relative position of the object right bottom corner $o2$ w.r.t. the object, $f5 = r''(o2) - r''(s)$
	\item Thus, the final $34$D feature representation is given by $f(s) = (f1, f2, f3, f4, f5)$.
\end{itemize}

The output of the network is a single scalar value representing the
cost-to-goal of the input state. Instead of learning the
cost-to-goal/value from scratch, we start with an initial value
estimate that is hardcoded (manhattan distance to goal in the $7$D
discrete grid) and the neural network approximator is used to learn a
residual on top of it. A similar trick was used in
\cmax{}~\cite{cmax}. The residual state value function
approximator was initialized to output $0$ for all $s \in
\statespace$. We use a similar architecture for the residual $Q$-value
function approximator but it takes as input the $34$D state feature
representation and outputs a vector in $\mathbb{R}^{|\actionspace|}$
(in our case, $\mathbb{R}^{14}$) to represent the cost-to-goal
estimate for each action $a \in \actionspace$. We also use the same
hardcoded value estimates as before in addition to the residual
approximator to construct the $Q$-values. All baselines and proposed
approaches use the same function approximator and same initial
hardcoded value estimates to ensure fair comparison. The value
function approximators are trained using mean squared loss.


The residual model learning baseline with neural network (NN) function
approximator uses the following architecture: $2$ hidden layers each
with $64$ units and all layers are followed by ReLU activations except
the last layer. The input of the network is the $34$D feature
representation of the state and a one-hot encoding of the action in
$\mathbb{R}^14$. The output of the network is the $7$D continuous
state which is added to the state predicted by the model $\Mhat$. The
loss function used to train the network is a simple mean squared
loss. The residual model learning baseline with K-Nearest Neighbor
regression approximator (KNN) uses a manhattan radius of $3$ in the
discrete $7$D state space. We compute the prediction by averaging the
next state residual vector observed in the past for any state that
lies within the radius of the current state. The averaged residual is
added to the next state predicted by model $\Mhat$ to obtain the
learned next state.


We use Adam optimizer~\cite{DBLP:journals/corr/KingmaB14} with a
learning rate of $0.001$ and a weight decay (L$2$ regularization
coefficient) of $0.001$ to train all the neural network function
approximators in all approaches. We use a batch size of $32$ for the
state value function approximators and a batch size of $128$ for the
$Q$-value function approximators. We perform $U = 3$ updates for state
value function and $U = 5$ updates for state-action value function for
each time step. We update the parameters of all neural network
approximators using a polyak averaging coefficient of $0.5$.


Finally, we use hindsight experience replay
trick~\cite{DBLP:conf/nips/AndrychowiczCRS17} in training all the
value function approximators with the probability of sampling any
future state in past trajectories as the goal set to $0.7$. This is
crucial as our cost function used is extremely sparse.

\clearpage
\newpage
\section{Appendix for Chapter~\ref{CHA:ILC}}
\label{sec:append-chapt-ilc}

\subsection{General Results}
\label{sec:general-results-1}

In this section, we will present general results that bound the cost suboptimality of
any time-varying controller $\Khat$ in terms of the norm differences
$||\KOPT_{t} - \Khat_{t}||$. Our first lemma makes use of
Assumption~\ref{assumption:stability} to show that if the norm differences
$||\KOPT_{t} - \Khat_{t}||$ are small, then the true system can be stable under
$\Khat$:
\begin{restatable}{lemma}{stabilityLemma}
  \label{lemma:stability}
  If Assumption~\ref{assumption:stability} holds and if $\Khat$ satisfies
  $||\KOPT_{i} - \Khat_{i}|| \le \frac{\delta}{2||B_{i}||}$ for all
  $i \in \{0, \cdots, H-1\}$, then we have
  \begin{equation}
    \label{eq:1}
    ||L_{t}(\Khat)|| \leq \left(1 - \frac{\delta}{2}\right)^{t+1} \leq e^{-\frac{\delta}{2}(t+1)}
  \end{equation}
\end{restatable}
\begin{proof}
  Observe that,
\begin{align*}
  ||L_t(\Khat)|| &= ||\prod_{i=0}^t M_i(\Khat)|| = ||\prod_{i=0}^t A_i
                   + B_i\Khat_i|| \\
  &= ||\prod_{i=0}^t A_i + B_i\KOPT_i + B_i(\Khat_i - \KOPT_i)|| =
    ||\prod_{i=0}^t M_i(\KOPT) + \Delta_i||
\end{align*}
where $\Delta_i = B_i(\Khat_i - \KOPT_i)$. Since the spectral norm is
sub-multiplicative we can see that
\begin{align*}
  ||\prod_{i=0}^t M_i(\KOPT) + \Delta_i|| &\leq \prod_{i=0}^t||M_i(\KOPT) +
                                        \Delta_i|| \\
  &\leq \prod_{i=0}^t(||M_i(\KOPT)|| + ||\Delta_i||)
\end{align*}
where we used the triangle inequality. Now note that
$||M_i(\KOPT)|| \leq 1-\delta$ from assumption~\ref{assumption:stability},
\begin{align*}
  \|\Delta_{i}\| &= \|B_{i}(\Khat_{i} - \KOPT_{i}) \| \leq \|B_{i}\|\|\Khat_{i} - \KOPT_{i}\| \\
                 &\leq \kappa \frac{\delta}{2\kappa} \\
  &\leq \frac{\delta}{2}
\end{align*}
The last inequality above is from our assumption on model errors in the lemma statement.
Combining all of this above, we get
\begin{align*}
  ||L_t(\Khat)|| \leq \prod_{i=0}^t\left(1 - \delta + \frac{\delta}{2}\right) \leq \left( 1-\frac{\delta}{2} \right)^{t+1}
\end{align*}
\end{proof}

The next lemma is very similar to the performance difference lemma that was
first proposed in~\cite{kakade2002approximately}. We borrow the version presented
in~\cite{fazel18} and extend it to the finite horizon setting below:
\begin{lemma}
  \label{lemma:performance-difference}
  Let $\xhat_0,
  \uhat_0, \cdots, \xhat_H, \uhat_H$ be the trajectory generated by
  controller
  $\Khat$ using the true dynamics such that $\xhat_0 = x_0$, $\uhat_t = \Khat_t\xhat_t$ for
  $t=0, \cdots, H-1$. Then
  we have:
  \begin{equation}
    \label{eq:2}
    \Vhat_0(x_0) - \VOPT_0(x_0) = \sum_{t=0}^{H-1} \AOPT_t(\xhat_t, \uhat_t) - \VOPT_H(\xhat_H)
  \end{equation}
  where $\Vhat_t$ is the cost-to-go using controller $\Khat$ from time
  step $t$, $\VOPT_t$ is the cost-to-go using the optimal controller $\KOPT$ from time
  step $t$, and $\AOPT_t(x, u) = \QOPT_t(x, u) - \VOPT_t(x)$ is the advantage of
  the controller $\KOPT$ at time step $t$. Furthermore, we have that for any $x$
  \begin{equation*}
    %\label{eq:3}
    \AOPT_t(x, \Khat_tx) = x^T(\Khat_t - \KOPT_t)^T(R + B_t^T\POPT_{t+1}B_t)
    (\Khat_t - \KOPT_t)x
  \end{equation*}
\end{lemma}
\begin{proof}
  For proof, we refer the readers to~\cite{fazel18}.
\end{proof}

We use the performance difference lemma, as stated above, in the finite horizon
LQR setup and make use of Lemma~\ref{lemma:stability} to establish the
suboptimality bound in terms of the norm differences $||\KOPT_{t} - \Khat_{t}||$:
\costTheorem*
\begin{proof}
  From Lemma~\ref{lemma:performance-difference} we have
  \begin{align*}
    A_{t}(\xhat_{t}, \Khat_{t}\xhat_{t}) &= \xhat_{t}^{T}(\Khat_{t} - \KOPT_{t})^{T}(R + B_{t}^{T}P_{t+1}B_{t})(\Khat_{t} - \KOPT_{t})\xhat_{t}
  \end{align*}
  We know $\xhat_{t} = L_{t-1}(\Khat)x_{0}$ and using the trace identity we get
  \begin{align*}
    A_{t}(\xhat_{t}, \Khat_{t}\xhat_{t}) &= \Tr(L_{t-1}(\Khat)x_{0}x_{0}^{T}(L_{t-1}(\Khat))^{T}(\Khat_{t} - \KOPT_{t})^{T}(R + B_{t}^{T}P_{t+1}B_{t})(\Khat_{t} - \KOPT_{t})) \\
                                         &\leq \|L_{t-1}(\Khat)x_{0}x_{0}^{T}\| \|R + B_{t}^{T}P_{t+1}B_{t}\| \|\Khat_{t} - \KOPT_{t}\|_{F}^{2} \\
    &\leq \|L_{t-1}(\Khat)\|^{2}\|x_{0}\|^{2} \|R + B_{t}^{T}P_{t+1}B_{t}\| \|\Khat_{t} - \KOPT_{t}\|_{F}^{2}
  \end{align*}
  We can bound $\|R + B_{t}^{T}P_{t+1}B_{t}\| \leq \Gamma^{3}$ and
  $\|\Khat_{t} - \KOPT_{t}\|_{F}^{2} \leq \min\{n, d\} \|\Khat_{t} - \KOPT_{t}\|^{2}$.
  We can also use Lemma~\ref{lemma:stability} to bound
  $\|L_{{t-1}}(\Khat)\| \leq \exp\left(-\frac{\delta}{2}t\right)$.
  Combining all
  of this above we get
  \begin{align*}
    A_{t}(\xhat_{t}, \Khat_{t}\xhat_{t}) &\leq \min\{n, d\}\Gamma^{3} \exp\left(-\delta t\right)\|\Khat_{t} - \KOPT_{t}\|^{2}\|x_{0}\|^{2}
  \end{align*}
  Summing over all time steps we
  obtain (using $d \leq n$)
  \begin{align*}
    \Vhat_0(x_0) - V_0(x_0) &\leq d\Gamma^{3}\|x_{0}\|^{2} \sum_{t=0}^{H-1} \exp\left(-\delta t\right)\|\Khat_{t} - \KOPT_{t}\|^{2}
  \end{align*}
\end{proof}

\subsection{Helpful Lemmas}
\label{sec:helpful-lemmas}

Before we dive into the results, let us present a helpful lemma
borrowed from \cite{mania19}:
\begin{lemma}
  Let $f_1, f_2$ be $\mu$-strongly convex twice differentiable
  functions. Let $x_1 = \arg\min_x f_1(x)$ and $x_2 = \arg\min_x
  f_2(x)$. Suppose $||\nabla f_1(x_2)|| \leq \epsilon$, then $||x_1 -
  x_2|| \leq \frac{\epsilon}{\mu}$
  \label{lemma:1}
\end{lemma}
\begin{proof}
  Taylor expanding $\nabla f_{1}$ we get
  \begin{align*}
    \nabla f_{1}(x_{2}) &= \nabla f_{1}(x_{1}) + \nabla^{2}f_{1}(\tilde{x})(x_{2} - x_{1}) \\
    &= \nabla^{2}f_{1}(\tilde{x})(x_{2} - x_{1})
  \end{align*}
  for some $\tilde{x} = tx_{1}+ (1-t)x_{2}$ where $t \in [0, 1]$. Thus we have
  \begin{align*}
    \|\nabla f_{1}(x_{2})\| = \|\nabla^{2}f_{1}(\tilde{x})(x_{2} - x_{1})\| \leq \epsilon
  \end{align*}
  But we know $\|\nabla^{2}f_{1}(\tilde{x})\| \geq \mu$ which gives us
  \begin{align*}
    \|x_{2} - x_{1}\| \leq \frac{\epsilon}{\mu}
  \end{align*}
\end{proof}

The next lemma is a useful fact about positive semi-definite matrices,
also from \cite{mania19},
\begin{lemma}
  \label{lemma:mania-appendix-original}
  Given matrices $A, \Ahat$ such that $\|A - \Ahat\| \leq \epsA$,
  and positive-semidefinite matrices $Q, S, \Shat$ we have
  \begin{equation}
    \label{eq:89}
    \|A^{T}Q(I + SQ)^{-1}A - \Ahat^{T}Q(I + \Shat Q)^{-1}\Ahat\| \leq \|A\|^{2}\|Q\|^{2}\|\Shat - S\| + 2\|A\|\|Q\|\epsA + \|Q\|\epsA^{2}
  \end{equation}
\end{lemma}
\begin{proof}
  We can rewrite the expression,
  \begin{align*}
    A^{T}Q(I + SQ)^{-1}A &- \Ahat^{T}Q(I + \Shat Q)^{-1}\Ahat = \\
                         &A^{T}Q(I + SQ)^{-1}(\Shat - S)Q(I+\Shat Q)^{-1}A - A^{T}Q(I + \Shat Q)^{-1}(\Ahat - A) \\
    &-(\Ahat - A)^{T}Q(I + \Shat Q)^{-1}A - (\Ahat - A)^{T}Q(I + \Shat Q)^{-1}(\Ahat - A)
  \end{align*}

  Now we make use of Lemma 7 from \cite{mania19} which states that for any two
  positive semidefinite matrices $M, N$ of the same dimension, we have
  $\|N(I + MN)^{-1}\| \leq \|N\|$. Thus, we have $\|Q(I+SQ)^{-1}\| \leq \|Q\|$
  and $\|Q(I + \Shat Q)^{-1}\| \leq \|Q\|$.

  Using the above facts we get,
  \begin{align*}
    \|A^{T}Q(I + SQ)^{-1}A - \Ahat^{T}Q(I + \Shat Q)^{-1}\Ahat\| &\leq \|A\|^{2}\|Q\|^{2}\|\Shat - S\| + 2\|A\|\|Q\|\epsA + \|Q\|\epsA^{2}
  \end{align*}
\end{proof}

Finally, we have a lemma that will be useful in proving ricatti
perturbation bounds,
\begin{lemma}
  Given positive semidefinite matrices $N_{1}, N_{2}, M$ of the same dimensions,
  we have
  \begin{equation}
    \label{eq:90}
    ||N_{1}(I + MN_{1})^{-1} - N_{2}(I + MN_{2})^{-1}|| \leq ||(I + MN_{1})^{-1}|| ||N_{1} - N_{2}|| ||(I + MN_{2})^{-1}||
  \end{equation}
  \label{lemma:stackexchange}
\end{lemma}
\begin{proof}
  We can rewrite the expression as,
  \begin{align*}
    &N_1(I+MN_1)^{-1}-N_2(I+MN_2)^{-1}\\
    &=\left[N_1(I+MN_1)^{-1}-N_1(I+MN_2)^{-1}\right]
    +\left[N_1(I+MN_2)^{-1}-N_2(I+MN_2)^{-1}\right]\\
    &=N_1(I+MN_1)^{-1}\left[(I+MN_2)-(I+MN_1)\right](I+MN_2)^{-1}
    +(N_1-N_2)(I+MN_2)^{-1}\\
    &=N_1(I+MN_1)^{-1}M(N_2-N_1)(I+MN_2)^{-1}
    +(N_1-N_2)(I+MN_2)^{-1}\\
    &=\left[I-N_1(I+MN_1)^{-1}M\right](N_1-N_2)(I+MN_2)^{-1} \\
    &=(I + N_{1}M)^{{-1}}(N_1-N_2)(I+MN_2)^{-1}
  \end{align*}
  The rest follows by taking norm on both sides, and using the submultiplicative
  property of the induced norm.
  \begin{align*}
    ||N_{1}(I + MN_{1})^{-1} - N_{2}(I + MN_{2})^{-1}|| &\leq ||(I + N_{1}M)^{-1}|| ||N_{1} - N_{2}|| ||(I + MN_{2})^{-1}|| \\
                                                        &= ||(I + N_{1}M)^{-T}||||N_{1} - N_{2}|| ||(I + MN_{2})^{-1}|| \\
    &= ||(I + MN_{1})^{-1}||||N_{1} - N_{2}|| ||(I + MN_{2})^{-1}||
  \end{align*}
\end{proof}

\subsection{Optimal Control with Misspecified Model Results}
\label{sec:cert-equiv-contr-2}

The next lemma, from \cite{mania19}, applies the above result to quadratic functions that
are observed in linear quadratic control:
\begin{lemma}
  \label{lemma:ce-quadratic}
  Define $f_1(x, u) = \frac{1}{2}u^TRu +
  \frac{1}{2}(A_1x+B_1u)^TP_1(A_1x+B_1u)$ and similarly define $f_2(x,
  u)$ where $R, P_1, P_2$ are positive-definite matrices. Let $K_1$ be
  such that $u_1 = \arg\min_u f_1(x, u) = K_1x$  for
  any vector $x$. Define the matrix $K_2$ in a similar fashion. Also,
  denote $\Gamma = 1 + \max\{||A_1||, \allowbreak||B_1||, ||P_1||,
  ||K_1||\}$. Suppose there exists $\epsA, \epsB, \epsP > 0$ (and
  $<\Gamma$) such that
  $||A_1 - A_2|| \leq \epsA$, $||B_1 - B_2|| \leq
  \epsB$, and $||P_1 - P_2|| \leq \epsP$. Then we have,
  \begin{equation}
    \label{eq:73}
    \|K_{1} - K_{2}\| \leq \frac{\Gamma^2\epsA + (3\Gamma^3 +
      2\Gamma^2)\epsB + 4(\Gamma^3 + \Gamma^2)\epsP}{\ubar{\sigma}(R)}
  \end{equation}
\end{lemma}

\begin{proof}
  Consider
  \begin{align*}
    \nabla_{u} f_{1}(x, u) &= (B_{1}^{T}P_{1}B_{1} + R)u + B_{1}^{T}P_{1}A_{1}x \\
    \nabla_{u} f_{2}(x, u) &= (B_{2}^{T}P_{2}B_{2} + R)u + B_{2}^{T}P_{2}A_{2}x
  \end{align*}

  Let us bound the difference $\|\nabla_{u} f_{1}(x,u) - \nabla_{u} f_{2}(x, u)\|$
  by bounding each term separately. First consider the term
  \begin{align*}
    \|B_{1}^{T}P_{1}B_{1} - B_{2}^{T}P_{2}B_{2}\| &= \|B_{1}^{T}P_{1}(B_{1} - B_{2}) + (B_{1} - B_{2})^{T}P_{1}B_{2} + B_{2}^{T}(P_{1} - P_{2})B_{2}\| \\
                                                  &\leq \|B_{1}^{T}P_{1}(B_{1} - B_{2})\| + \|(B_{1} - B_{2})^{T}P_{1}B_{2}\| + \|B_{2}^{T}(P_{1} - P_{2})B_{2}\| \\
                                                  &\leq \Gamma^{2}\epsB + \Gamma\epsB(\Gamma + \epsB) + (\Gamma + \epsB)^{2}\epsP \\
    &\leq \Gamma^2(3\epsB + 4\epsP)
  \end{align*}
  where we used the fact that $\|B_{2}\| \leq \Gamma + \epsB$.
  We can similarly bound the term
  \begin{align*}
    \|B_{1}^{T}P_{1}A_{1} - B_{2}^{T}P_{2}A_{2}\| \leq
    \Gamma^{2}(\epsA + 2\epsB + 4\epsP)
  \end{align*}


  Thus, we have for any vector $x$ such that $\|x\| \leq 1$
  \begin{align*}
    \|\nabla_{u} f_{1}(x,u) - \nabla_{u} f_{2}(x, u)\| \leq
    \Gamma^2(3\epsB + 4\epsP)\|u\| + \Gamma^2(\epsA + 2\epsB + 4\epsP)
  \end{align*}
  Substituting $u=u_{1}$ we get
  \begin{align*}
    \|\nabla_{u} f_{2}(x, u_{1})\| \leq \Gamma^2(3\epsB + 4\epsP)\|u_1\| + \Gamma^2(\epsA + 2\epsB + 4\epsP)
  \end{align*}

  We can bound $\|u_{1}\| \leq \|K_{1}\|\|x\| \leq \|K_{1}\| \leq \Gamma$. Then
  from Lemma~\ref{lemma:1} we have,
  \begin{align*}
    \|u_{1} - u_{2}\| &\leq \frac{\Gamma^3(3\epsB + 4\epsP) +
                        \Gamma^2(\epsA + 2\epsB + 4\epsP)}{\ubar{\sigma}(R)} \\
    \|K_{1} - K_{2}\| &\leq \frac{\Gamma^2\epsA + (3\Gamma^3 +
      2\Gamma^2)\epsB + 4(\Gamma^3 + \Gamma^2)\epsP}{\ubar{\sigma}(R)}
  \end{align*}
\end{proof}

Now we will prove Lemma~\ref{lemma:ce},
\ceLemma*
\begin{proof}
  Use Assumption~\ref{assumption:singularvalue} and
  Lemma~\ref{lemma:ce-quadratic} for every $t=0, \cdots, H-1$ with
  $\epsP = \fCE_{t+1}(\epsA, \epsB)$ and choosing $\epsilon_t =
  \max\{\epsA, \epsB, \fCE_{t+1}(\epsA, \epsB)\}$.
\end{proof}

All that is left is to prove Theorem~\ref{theorem:ce} which we will do
now,
\theoremCE*
\begin{proof}
  We know $\POPT_{t}$ satisfies,
\begin{align*}
  \POPT_{t} &= Q + A_t^{T}\POPT_{{t+1}}A_t - A_t^{T}\POPT_{t+1}B_t(R + B_t^{T}\POPT_{{t+1}}B_t)^{-1}B_t^{T}\POPT_{t+1}A_t \\
  &= Q + A_t^{T}\POPT_{t+1}(I + B_tR^{-1}B_t^{T}\POPT_{t+1})^{-1}A_t
\end{align*}
where we used the matrix inversion lemma.

Similarly we have,
\begin{align*}
  \PCE_{t} &= Q + \Ahat_t^{T}\PCE_{t+1}(I + \Bhat_t R^{-1}\Bhat_t^{T}\PCE_{t+1})^{{-1}}\Ahat_t
\end{align*}

Consider the difference,
\begin{align*}
  \POPT_{t} - \PCE_{t} &= A_t^{T}\POPT_{t+1}(I + B_tR^{-1}B_t^{T}\POPT_{t+1})^{-1}A_t - \Ahat_t^{T}\PCE_{t+1}(I + \Bhat_t R^{-1}\Bhat_t^{T}\PCE_{t+1})^{{-1}}\Ahat_t \\
                    &= A_t^{T}\POPT_{t+1}(I + B_tR^{-1}B_t^{T}\POPT_{t+1})^{-1}A_t - \Ahat_t^{T}\POPT_{t+1}(I + \Bhat_t R^{-1}\Bhat_t^{T}\POPT_{t+1})^{{-1}}\Ahat_t \\
  &+ \Ahat_t^{T}\left(\POPT_{t+1}(I + \Bhat_t R^{-1}\Bhat_t^{T}\POPT_{t+1})^{{-1}} - \PCE_{t+1}(I + \Bhat_t R^{-1}\Bhat_t^{T}\PCE_{t+1})^{{-1}} \right)\Ahat_t
\end{align*}
To bound the above expression, we will make use of
Lemma~\ref{lemma:mania-appendix-original} with $S = B_tR^{-1}B_t^{T}$,
$\Shat = \Bhat_t R^{-1}\Bhat_t^{T}$,
$Q = \POPT_{t+1}$ and observing that
$\|\Shat - S\| \leq 2\|B_t\|\|R^{-1}\|\epsB + \|R^{-1}\|\epsB^{2}$ we obtain

\begin{align*}
  \|\PCE_{t} - \POPT_{t}\| \leq& \|A_t\|^{2}\|\POPT_{t+1}\|^{2}(2\|B_t\|\|R^{-1}\|\epsB + \|R^{-1}\|\epsB^{2}) + 2\|A_t\|\|\POPT_{t+1}\|\epsA + \|\POPT_{t+1}\|\epsA^{2} \\
  &+ \|\Ahat_t^{T}\left(\POPT_{t+1}(I + \Bhat_t R^{-1}\Bhat_t^{T}\POPT_{t+1})^{{-1}} - \PCE_{t+1}(I + \Bhat_t R^{-1}\Bhat_t^{T}\PCE_{t+1})^{{-1}} \right)\Ahat_t\|
\end{align*}

%\iffalse % START CONDITION NUMBER PROOF
All that remains is to bound the second expression. We will use
Lemma~\ref{lemma:stackexchange} with $N_{1} = \POPT_{t+1}$, $N_{2} = \PCE_{t+1}$ and
$M = \Bhat_t R^{{-1}}\Bhat_t^{T}$ gives us,
\begin{align*}
  &||\POPT_{t+1}(I + \Bhat_t R^{-1}\Bhat_t^{T}\POPT_{t+1})^{{-1}} - \PCE_{t+1}(I + \Bhat_t R^{-1}\Bhat_t^{T}\PCE_{t+1})^{{-1}}|| \\
  &\leq ||(I + \Bhat_t R^{{-1}}\Bhat_t^{T}\POPT_{t+1})^{-1}|| ||\POPT_{t+1} - \PCE_{{t+1}}|| ||(I + \Bhat_t R^{{-1}}\Bhat_t^{T}\PCE_{t+1})^{-1}||
\end{align*}

Thus, we have
\begin{align*}
  ||\POPT_{t} - \PCE_{t}|| &\leq  \|A_t\|^{2}\|\POPT_{t+1}\|^{2}(2\|B_t\|\|R^{-1}\|\epsB + \|R^{-1}\|\epsB^{2}) + 2\|A_t\|\|\POPT_{t+1}\|\epsA + \|\POPT_{t+1}\|\epsA^{2} \\
  &+ \|\Ahat_t\|^{2}||(I + \Bhat_t R^{{-1}}\Bhat_t^{T}\POPT_{t+1})^{-1}|| ||\POPT_{t+1} - \PCE_{{t+1}}|| ||(I + \Bhat_t R^{{-1}}\Bhat_t^{T}\PCE_{t+1})^{-1}||
\end{align*}

Observe that we can bound
\begin{align*}
  ||(I + \Bhat_t R^{{-1}}\Bhat_t^{T}\POPT_{t+1})^{-1}|| &= ||(\POPT_{t+1})^{-1}\POPT_{t+1}(I + \Bhat_t R^{{-1}}\Bhat_t^{T}\POPT_{t+1})^{-1}|| \\
                                                &\leq ||(\POPT_{t+1})^{-1}|| ||\POPT_{t+1}(I + \Bhat_t R^{{-1}}\Bhat_t^{T}\POPT_{t+1})^{-1}|| \\
  &\leq ||(\POPT_{t+1})^{-1}|| ||\POPT_{t+1}|| = \kappa_{\POPT_{t+1}}
\end{align*}
where $\kappa_{\POPT_{t+1}}$ is the condition number of the matrix $\POPT_{t+1}$. This
gives us the bound
\begin{align*}
  ||\POPT_{t} - \PCE_{t}|| &\leq  \|A_t\|^{2}\|\POPT_{t+1}\|^{2}(2\|B_t\|\|R^{-1}\|\epsB + \|R^{-1}\|\epsB^{2}) + 2\|A_t\|\|\POPT_{t+1}\|\epsA + \|\POPT_{t+1}\|\epsA^{2} \\
                        &+
                          \|\Ahat_t\|^{2}\kappa_{\POPT_{t+1}}\kappa_{\PCE_{t+1}}
                          ||\POPT_{t+1} - \PCE_{{t+1}}||
\end{align*}

Using the fact that
$||\Ahat_t||^{2} \leq (\|A_t\| + \epsA)^2$ gives us
\begin{align}
  \label{eq:92}
  ||\POPT_{t} - \PCE_{t}|| &\leq  \|A_t\|^{2}\|\POPT_{t+1}\|^{2}(2\|B_t\|\|R^{-1}\|\epsB + \|R^{-1}\|\epsB^{2}) + 2\|A_t\|\|\POPT_{t+1}\|\epsA + \|\POPT_{t+1}\|\epsA^{2} \nonumber\\
                        &+ (\|A_t\| +  \epsA)^2\kappa_{\POPT_{t+1}}\kappa_{\PCE_{t+1}} ||\POPT_{t+1} - \PCE_{{t+1}}||
\end{align}
%\fi % END CONDITION NUMBER PROOF

If $\epsA, \epsB$ are small enough that $\|\POPT_{t+1} -
\PCE_{t+1}\|\|\POPT_{t+1}\| \leq 1$ then we can bound
\begin{align*}
  \|(\PCE_{t+1})^{-1} - (\POPT_{t+1})^{-1}\| &\leq
  \frac{\|(\POPT_{t+1})^{-1}\|}{1 - \|(\POPT_{t+1})^{-1}\|\|\POPT_{t+1}
                                               - \PCE_{t+1}\|} \\
  &\leq \frac{\|(\POPT_{t+1})^{-1}\|\|\POPT_{t+1}\|}{\|\POPT_{t+1}\| -
    \|(\POPT_{t+1})^{-1}\|} \\
  &\leq \frac{\kappa_{\POPT_{t+1}}}{\|\POPT_{t+1}\|^2 - \kappa_{\POPT_{t+1}}}
\end{align*}
The above result is from~\cite{horn12} (Section 5.8 page 381). Now we
can bound the condition number $\kappa_{\PCE_{t+1}}$ by observing that
$\|(\PCE_{t+1})^{-1}\| \leq \|(\POPT_{t+1})^{-1}\| +
\frac{\kappa_{\POPT_{t+1}}}{\|\POPT_{t+1}\|^2 - \kappa_{\POPT_{t+1}}}$
and $\|\PCE_{t+1}\| \leq \|\POPT_{t+1}\| + \|\PCE_{t+1} -
\POPT_{t+1}\| \leq \|\POPT_{t+1}\| + \frac{1}{\|\POPT_{t+1}\|}$ giving
us
\begin{align*}
  \kappa_{\POPT_{t+1}}\kappa_{\PCE_{t+1}} &= \kappa_{\POPT_{t+1}}\|(\PCE_{t+1})^{-1}\|\|\PCE_{t+1}\| \leq \kappa_{\POPT_{t+1}}(\|(\POPT_{t+1})^{-1}\| +
\frac{\kappa_{\POPT_{t+1}}}{\|\POPT_{t+1}\|^2 -
                        \kappa_{\POPT_{t+1}}})(\|\POPT_{t+1}\| +
                        \frac{1}{\|\POPT_{t+1}\|}) \\
  &= \kappa_{\POPT_{t+1}}^2 +
    \frac{\kappa_{\POPT_{t+1}}^2}{\|\POPT_{t+1}\|^2} + \frac{\kappa_{\POPT_{t+1}}^2}{\|\POPT_{t+1}\|^2 -
                        \kappa_{\POPT_{t+1}}}(\|\POPT_{t+1}\| +
                        \frac{1}{\|\POPT_{t+1}\|})
\end{align*}
Denoting $c_{\POPT_{t+1}}$ as the right hand side expression in the
above inequality we get the desired result.The example that
realizes the upper bound is given in Appendix~\ref{sec:scalar-example-that}.

\end{proof}

\subsection{Note on Assumption~\ref{assumption:psd}}
\label{sec:assumpt-refass}

Consider the cost-to-go matrix $\PILC_t$ given by
\begin{align*}
  \PILC_{t} &= Q + \Ahat_t^{T}\PILC_{{t+1}}A_t - \Ahat_t^{T}\PILC_{t+1}B_t(R + \Bhat_t^{T}\PILC_{{t+1}}B_t)^{-1}\Bhat_t^{T}\PILC_{t+1}A_t \\
  &= Q + \Ahat_t^{T}\PILC_{t+1}(I + B_tR^{-1}\Bhat_t^{T}\PILC_{t+1})^{-1}A_t
\end{align*}
and the cost-to-go from any state $x$ is given by
\begin{align*}
  V_t(x) = x^T\PILC_tx
\end{align*}
Since this is a quadratic, for it to be convex (and thus, have a
minima) we require the leading
coefficient to be positive semi-definite. In other words, $\PILC_t$
should have eigenvalues with non-negative real parts. Assuming
$\PILC_{t+1}$ to be positive semi-definite, and observing the fact
that $Q$ is a positive semi-definite matrix, we require that
$B_tR^{-1}\Bhat_t^T$ to have eigenvalues with non-negative real parts
for $\PILC_t$ to be positive semi-definite. Note that this is
trivially satisfied for \MM{} as the leading coefficient there
contains a similar term $B_tR^{-1}B_t^T$ which is positive
semi-definite.

Intuitively, if $B_tR^{-1}\Bhat_t^T$ does not have eigenvalues with
non-negative real parts, then the resulting quadratic cost-to-go
function need not be convex, and \ILC{} will not converge.

\subsection{Iterative Learning Control Results}
\label{sec:iter-learn-contr}

Our first lemma derives a similar result as
Lemma~\ref{lemma:ce-quadratic} but for the iterative learning control
setting,
\begin{lemma}
  \label{lemma:ilc-quadratic}
  Given functions $f_{1}(x, u)$ and $f_{2}(x, u)$ such that
  $\nabla_{u} f_{1}(x, u) = (B_{1}^{T}P_{1}B_{1} + R)u + B_{1}^{T}P_{1}A_{1}x$
  and
  $\nabla_{u} f_{2}(x, u) = (B_{2}^{T}P_{2}B_{1} + R)u + B_{2}^{T}P_{2}A_{1}x$
  where $R, P_{1}, P_{2}$ are positive-definite matrices. Let $K_{1}$ and
  $K_{2}$ be unique matrices such that $\nabla_{u} f_{1}(x, K_{1}x) = 0$ and
  $\nabla_{u} f_{2}(x, K_{2}x) = 0$ for any
  vector $x$. Also,
  denote $\Gamma = 1 + \max\{||A_1||, ||B_1||, ||P_1||,
  ||K_1||\}$. Suppose there exists $\epsA, \epsB, \epsP > 0$ (and $<\Gamma$) such that
  $||A_1 - A_2|| \leq \epsA$, and $||B_1 - B_2|| \leq
  \epsB$, and $||P_1 - P_2|| \leq \epsP$. Then we have,
  \begin{equation}
    \label{eq:81}
    \|K_{1} - K_{2}\| \leq \frac{2\Gamma^3(\epsB + 2\epsP)}{\ubar{\sigma}(R)}
  \end{equation}
\end{lemma}
\begin{proof}
  Let us bound the difference $\|\nabla_{u} f_{1}(x,u) - \nabla_{u} f_{2}(x, u)\|$
  by bounding each term separately. First consider the term
  \begin{align*}
    \|B_{1}^{T}P_{1}B_{1} - B_{2}^{T}P_{2}B_{1}\| &= \|(B_{1} - B_{2})^{T}P_{1}B_{1} + B_{2}^{T}(P_{1} - P_{2})B_{1}\| \\
                                                  &\leq \Gamma^{2}\epsB + \Gamma(\Gamma + \epsB)\epsP \\
    &\leq \Gamma^2(\epsB + 2\epsP)
  \end{align*}
  where we used the fact that $\|B_{2}\| \leq \Gamma + \epsB$.
  We can similarly bound the term
  \begin{align*}
    \|B_{1}^{T}P_{1}A_{1} - B_{2}^{T}P_{2}A_{1}\| \leq \Gamma^{2}(\epsB + 2\epsP)
  \end{align*}

  Thus, we have for any vector $x$ such that $\|x\| \leq 1$
  \begin{align*}
    \|\nabla_{u} f_{1}(x,u) - \nabla_{u} f_{2}(x, u)\| \leq \Gamma^{2}(\epsB + 2\epsP)(\|u\| + 1)
  \end{align*}
  Substituting $u=u_{1}$ we get
  \begin{align*}
    \|\nabla_{u} f_{2}(x, u_{1})\| \leq \Gamma^{2}(\epsB + 2\epsP)(\|u_{1}\| + 1)
  \end{align*}

  We can bound $\|u_{1}\| \leq \|K_{1}\|\|x\| \leq \|K_{1}\| \leq \Gamma$. Then
  from Lemma~\ref{lemma:1} we have,
  \begin{align*}
    \|u_{1} - u_{2}\| &\leq \frac{\Gamma^{2}(\epsB + 2\epsP)(\Gamma+1)}{\ubar{\sigma}(R)} \\
    \|K_{1} - K_{2}\| &\leq \frac{2\Gamma^{3}(\epsB + 2\epsP)}{\ubar{\sigma}(R)}
  \end{align*}
\end{proof}

Now we will prove Lemma~\ref{lemma:ilc},
\ilcLemma*
\begin{proof}
  Use Assumption~\ref{assumption:singularvalue} and
  Lemma~\ref{lemma:ilc-quadratic} for $t = 0, \cdots, H-1$ with
  $\epsP = \fILC_{t+1}(\epsA, \epsB)$ and choosing $\epsilon_t =
  \max\{\epsA, \epsB, \fILC_{t+1}(\epsA, \epsB)\}$.
\end{proof}

Our final task is to prove Theorem~\ref{theorem:ilc},
\ilcTheorem*
\begin{proof}
  We know $\PILC_{t}$ satisfies,
\begin{align*}
  \PILC_{t} &= Q + \Ahat_t^{T}\PILC_{{t+1}}A_t - \Ahat_t^{T}\PILC_{t+1}B_t(R + \Bhat_t^{T}\PILC_{{t+1}}B_t)^{-1}\Bhat_t^{T}\PILC_{t+1}A_t \\
  &= Q + \Ahat_t^{T}\PILC_{t+1}(I + B_tR^{-1}\Bhat_t^{T}\PILC_{t+1})^{-1}A_t
\end{align*}
where we used the matrix inversion lemma.

Consider the difference,
\begin{align*}
  \POPT_{t} - \PILC_{t} &= A_t^{T}\POPT_{t+1}(I + B_tR^{-1}B_t^{T}\POPT_{t+1})^{-1}A_t - \Ahat_t^{T}\PILC_{t+1}(I + B_tR^{-1}\Bhat_t^{T}\PILC_{t+1})^{-1}A_t \\
                      &= A^{T}\POPT_{t+1}(I + B_tR^{-1}B_t^{T}\POPT_{t+1})^{-1}A_t - \Ahat_t^{T}\POPT_{t+1}(I + B_tR^{-1}\Bhat_t^{T}\POPT_{t+1})^{-1}A_t \\
  &+ \Ahat_t^{T}\left(\POPT_{t+1}(I + B_tR^{-1}\Bhat_t^{T}\POPT_{t+1})^{-1} - \PILC_{t+1}(I + B_tR^{-1}\Bhat_t^{T}\PILC_{t+1})^{-1}\right)A_t
\end{align*}

Here again we can use Lemma~\ref{lemma:mania-appendix-original} with
$S = B_tR^{-1}B_t^{T}$, $\Shat = B_tR^{-1}\Bhat_t^{T}$, $Q = \POPT_{t+1}$ and observing that
$||\Shat - S|| \leq ||B_t||||R^{-1}||\epsB$ to get
\begin{align*}
  ||\PILC_{t} - \POPT_{t}|| &\leq ||A_t||^{2}||\POPT_{t+1}||^{2}||B_t||||R^{-1}||\epsB + ||A_t||||\POPT_{t+1}||\epsA \\
  &+ ||A_t||||\Ahat_t||||\POPT_{t+1}(I + B_tR^{-1}\Bhat_t^{T}\POPT_{t+1})^{-1} - \PILC_{t+1}(I + B_tR^{-1}\Bhat_t^{T}\PILC_{t+1})^{-1}||
\end{align*}

Here again we use Lemma~\ref{lemma:stackexchange} to bound the second expression
giving us
\begin{align*}
  ||\PILC_{t} - \POPT_{t}|| &\leq ||A_t||^{2}||\POPT_{t+1}||^{2}||B_t||||R^{-1}||\epsB + ||A_t||||\POPT_{t+1}||\epsA \\
  &+ ||A_t||||\Ahat_t|| ||(I + B_tR^{-1}\Bhat_t^{T}\POPT_{t+1})^{-1}|| ||\PILC_{t+1} - \POPT_{t+1}|| ||(I + B_tR^{-1}\Bhat_t^{T}\PILC_{t+1})^{-1}||
\end{align*}

This can be rewritten as the final bound,
\begin{align}
  \label{eq:91}
  ||\POPT_{t} - \PILC_{t}|| &\leq ||A_t||^{2}||\POPT_{t+1}||^{2}||B_t||||R^{-1}||\epsB + ||A_t||||\POPT_{t+1}||\epsA \nonumber\\
  &+ (||A_t||^{2} + \epsA||A_t||) \kappa_{\POPT_{t+1}}\kappa_{\PILC_{t+1}} ||\POPT_{t+1} - \PILC_{t+1}||
\end{align}

The constant $c_{\POPT_{t+1}}$ can be derived very similarly as we
have done in the proof of Theorem~\ref{theorem:ce}. The example that
realizes the upper bound is given in Appendix~\ref{sec:scalar-example-that}.
\end{proof}

\subsection{Scalar Example that Realizes Upper Bounds}
\label{sec:scalar-example-that}

\subsubsection{General Formulation}
\label{sec:general-formulation}
Consider a $1$D linear dynamical system given by,
\begin{equation}
  \label{eq:10}
  x_t = ax_t + bu_t
\end{equation}
where $x_t, u_t, a, b \in \reals$. The cost function is given by,
\begin{equation}
  \label{eq:11}
  V_0(x_0) = \sum_{t=0}^{H-1} qx_t^2 + ru_t^2 + qx_H^2
\end{equation}
We are given access to an approximate model specified using $\ahat,
\bhat \in \reals$.

The optimal cost-to-go is specified using
\begin{align}
  \label{eq:12}
  &\popt_H = q \\
  &\popt_t = q + \frac{a^2\popt_{t+1}}{1 + b^2r^{-1}\popt_{t+1}} = q + \frac{a^2r\popt_{t+1}}{r + b^2\popt_{t+1}}
\end{align}

For \MM{}, the cost-to-go is specified using
\begin{align}
  \label{eq:13}
  &\pce_H = q \\
  &\pce_t = q + \frac{\ahat^2r\pce_{t+1}}{r + \bhat^2\pce_{t+1}}
\end{align}

For ILC, the cost-to-go is specified using
\begin{align}
  \label{eq:8}
  &\pilc_h = q \\
  &\pilc_t = q + \frac{a\ahat r\pilc_{t+1}}{r + b\bhat\pilc_{t+1}}
\end{align}

In the next two subsections, we will show that an example dynamical
system where $\bhat = 0$, i.e. the approximate model thinks that the
system is not controllable will realize the worst case upper bounds
for both \MM{} and ILC as presented in Theorems~\ref{theorem:ce}
and~\ref{theorem:ilc} respectively.

\subsubsection{Optimal Control with Misspecified Model}
\label{sec:cert-equiv-contr-1}

Consider the difference
\begin{align*}
  \popt_t - \pce_t &= \frac{a^2r\popt_{t+1}}{r + b^2\popt_{t+1}} -
                       \frac{\ahat^2r\pce_{t+1}}{r +
                       \bhat^2\pce_{t+1}} \\
  &= \left( \frac{a^2r\popt_{t+1}}{r + b^2\popt_{t+1}} -
    \frac{\ahat^2r\popt_{t+1}}{r + \bhat^2\popt_{t+1}} \right) +
    \left( \frac{\ahat^2r\popt_{t+1}}{r + \bhat^2\popt_{t+1}} - \frac{\ahat^2r\pce_{t+1}}{r +
                       \bhat^2\pce_{t+1}} \right)
\end{align*}
Let us look at each term separately. The first term can be simplified
as
\begin{equation}
  \label{eq:17}
  \left( \frac{a^2r\popt_{t+1}}{r + b^2\popt_{t+1}} -
    \frac{\ahat^2r\popt_{t+1}}{r + \bhat^2\popt_{t+1}} \right) =
                                                                 \frac{\popt_{t+1}(a^2-
                                                                 \ahat^2)}{1
                                                                 +
                                                                 \bhat^2r^{-1}\popt_{t+1}}
                                                                 +
                                                                 \frac{a^2r^{-1}(\bhat^2
                                                                 -
                                                                 b^2)(\popt_{t+1})^2}{(1
                                                                 +
                                                                 b^2r^{-1}\popt_{t+1})(1
                                                                 +
                                                                 \bhat^2r^{-1}\popt_{t+1})}
\end{equation}
Similarly, the second term can be simplified as
\begin{equation}
  \label{eq:9}
  \left( \frac{\ahat^2r\popt_{t+1}}{r + \bhat^2\popt_{t+1}} - \frac{\ahat^2r\pce_{t+1}}{r +
                       \bhat^2\pce_{t+1}} \right) =
                   \frac{\ahat^2(\popt_{t+1} - \pce_{t+1})}{(1 +
                     \bhat^2r^{-1}\popt_{t+1})(1 + \bhat^2r^{-1}\pce_{t+1})}
\end{equation}

Now, consider the example dynamical system where $a - \ahat = \epsa$,
$b - \bhat = \epsb$, and $\bhat = 0$. Our upper bound in
Theorem~\ref{theorem:ce} states that,
\begin{equation}
  \label{eq:16}
  |\popt_{t} - \pce_t| \leq a^2r^{-1}(\popt_{t+1})^2(2b\epsb +
  \epsb^2) + \popt_{t+1}(2a\epsa + \epsa^2) + (a +
  \epsa)^2|\popt_{t+1} - \pce_{t+1}|
\end{equation}
For the example system equation~\eqref{eq:17} simplifies to,
\begin{align*}
  \frac{\popt_{t+1}(a^2-
                                                                 \ahat^2)}{1
                                                                 +
                                                                 \bhat^2r^{-1}\popt_{t+1}}
                                                                 +
                                                                 \frac{a^2r^{-1}(\bhat^2
                                                                 -
                                                                 b^2)(\popt_{t+1})^2}{(1
                                                                 +
                                                                 b^2r^{-1}\popt_{t+1})(1
                                                                 +
                                                                 \bhat^2r^{-1}\popt_{t+1})}
  = \popt_{t+1}(2a\epsa + \epsa^2)
                                                                 +
                                                                 \frac{a^2r^{-1}(\popt_{t+1})^2(2b\epsb
  + \epsb^2)}{(1
                                                                 +
                                                                 b^2r^{-1}\popt_{t+1})}
\end{align*}
which matches the first two terms in the upper bound
(equation~\eqref{eq:16}) upto a constant. Now, let's look at how
equation~\eqref{eq:9} simplifies
\begin{align*}
  \frac{\ahat^2(\popt_{t+1} - \pce_{t+1})}{(1 +
                     \bhat^2r^{-1}\popt_{t+1})(1 +
  \bhat^2r^{-1}\pce_{t+1})} = (a + \epsa)^2(\popt_{t+1} - \pce_{t+1})
\end{align*}
which matches the last term in the upper bound
(equation~\eqref{eq:16}) exactly. Thus, we found an example where
$|\popt_t - \pce_t|$ matches the upper bound specified in
Theorem~\ref{theorem:ce} upto a constant.

\subsubsection{Iterative Learning Control}
\label{sec:iter-learn-contr-2}

Consider the difference
\begin{align*}
  \popt_t - \pilc_t &= \frac{a^2r\popt_{t+1}}{r + b^2\popt_{t+1}} -
                       \frac{a\ahat r\pilc_{t+1}}{r +
                      b\bhat\pilc_{t+1}} \\
  &= \left( \frac{a^2r\popt_{t+1}}{r + b^2\popt_{t+1}} -
    \frac{a\ahat r\popt_{t+1}}{r + b\bhat\popt_{t+1}} \right) + \left(
    \frac{a\ahat r\popt_{t+1}}{r + b\bhat\popt_{t+1}} - \frac{a\ahat r\pilc_{t+1}}{r +
                      b\bhat\pilc_{t+1}}\right)
\end{align*}
Once again let us look at each term separately. The first term can be
simplified as
\begin{equation}
  \label{eq:20}
  \left( \frac{a^2r\popt_{t+1}}{r + b^2\popt_{t+1}} -
    \frac{a\ahat r\popt_{t+1}}{r + b\bhat\popt_{t+1}} \right) =
  \frac{a\popt_{t+1}(a - \ahat)}{(1 + b\bhat r^{-1}\popt_{t+1})} +
  \frac{a^2br^{-1}(\popt_{t+1})^2(\bhat - b)}{(1 + b^2r^{-1}\popt_{t+1})(1
    + b\bhat r^{-1}\popt_{t+1})}
\end{equation}
Similarly, the second term can be simplified as
\begin{equation}
  \label{eq:18}
  \left(
    \frac{a\ahat r\popt_{t+1}}{r + b\bhat\popt_{t+1}} - \frac{a\ahat r\pilc_{t+1}}{r +
                      b\bhat\pilc_{t+1}}\right) =
                  \frac{a\ahat(\popt_{t+1} - \pilc_{t+1})}{(1 + b\bhat
                    r^{-1}\popt_{t+1})(1 + b\bhat r^{-1}\pilc_{t+1})}
\end{equation}
Similar to \MM{} in the previous section, consider the example dynamical
system where $a - \ahat = \epsa$, $b - \bhat = \epsb$ and $\bhat =
0$. Our upper bound in Theorem~\ref{theorem:ilc} states that
\begin{equation}
  \label{eq:19}
  |\popt_t - \pilc_t| \leq a^2(\popt_{t+1})^2br^{-1}\epsb +
  a\popt_{t+1}\epsa + a(a + \epsa)|\popt_{t+1} - \pilc_{t+1}|
\end{equation}
For the example dynamical system, equation~\eqref{eq:20} simplifies to
\begin{align*}
  \frac{a\popt_{t+1}(a - \ahat)}{(1 + b\bhat r^{-1}\popt_{t+1})} +
  \frac{a^2br^{-1}(\popt_{t+1})^2(\bhat - b)}{(1 + b^2r^{-1}\popt_{t+1})(1
    + b\bhat r^{-1}\popt_{t+1})} = a\popt_{t+1}\epsa +
  \frac{a^2(\popt_{t+1})^2br^{-1}\epsb}{(1 + b^2r^{-1}\popt_{t+1})}
\end{align*}
which matches the first two terms in the upper bound
(equation~\eqref{eq:19}) upto a constant. Now, let's look at how
equation~\eqref{eq:18} simplifies
\begin{align*}
  \frac{a\ahat(\popt_{t+1} - \pilc_{t+1})}{(1 + b\bhat
                    r^{-1}\popt_{t+1})(1 + b\bhat r^{-1}\pilc_{t+1})}
  = a(a + \epsa)(\popt_{t+1} - \pilc_{t+1})
\end{align*}
which matches the last term in the upper bound
(equation~\eqref{eq:19}) exactly. Thus, we found that the same example
also matches the upper bound specified in Theorem~\ref{theorem:ilc}
upto a constant.

\subsection{Experiment Details}
\label{sec:experiment-details}

\subsubsection{Linear Dynamical System with Approximate Model}
\label{sec:line-dynam-syst-1}

We use a horizon $H = 10$ and initial state $x_0 = \begin{bmatrix}0.1
  \\ 0.1 \end{bmatrix}$.

\subsubsection{Nonlinear Inverted Pendulum with Misspecified Mass}
\label{sec:nonl-invert-pend}

For the second experiment, we use the nonlinear dynamical system of an inverted
pendulum. The state space is specified by $x =
\begin{bmatrix}
  \theta \\
  \dot{\theta}
\end{bmatrix} \in \reals^{2}
$ where $\theta$ is the angle between the pendulum and the vertical axis. The
control input is $u = \tau \in \reals$ specifying the torque $\tau$ to be
applied at the base of the pendulum. The dynamics of the system are
given by the ODE,
%\begin{align*}
$\ddot{\theta} = \frac{\bar{\tau}}{m\ell^{2}} - \frac{g\sin(\theta)}{\ell}$
%\end{align*}
where $m$ is the mass of the pendulum, $\ell$ is the length of the pendulum, $g$
is the acceleration due to gravity, and
$\bar{\tau} = \max(\tau_{\min}, \min(\tau_{\max}, \tau))$ is the clipped torque
based on torque limits. We use $\ell = 1$m, $\tau_{\max} = 8$Nm,
$\tau_{\min} = -8$Nm, and $m = 1$kg.

We use a per time step cost function defined as $c(\theta, \tau) =
0.1\tau^{2} + \theta^{2}$
where $\theta \in [-\pi, \pi]$, an initial state $x_{0} =
\begin{bmatrix}
  \frac{\pi}{2} \\ 0.5
\end{bmatrix}
$, and a horizon $H = 20$. For all algorithms, we start with an
initial control sequence consisting of zero torques for the entire horizon.

\subsubsection{Nonlinear Planar Quadrotor Control in Wind}
\label{sec:nonl-plan-quadr-1}

In our final experiment, we compare \MM{} and \ILC{} on a planar quadrotor control
task in the presence of wind. The quadrotor is controlled using two propellers
that provide upward
thrusts $(u_{1}, u_{2})$ and allows movement in the $3$D planar space
described as
$(p_{x}, p_{y}, \theta)$ where $p_{x}, p_{y}$ are X, Y positions, and
$\theta$ is the yaw of the quadrotor. The dynamics of the planar quadrotor
is specified
using a state vector $x \in \reals^{6}$, control input $u \in \reals^{2}$ as
\begin{align*}
  x =
  \begin{bmatrix}
    p_{x} \\ p_{y} \\ \theta \\ \dot{p}_{x} \\ \dot{p}_{y} \\ \dot{\theta}
  \end{bmatrix}, u =
  \begin{bmatrix}
    u_{1} \\ u_{2}
  \end{bmatrix},
  \dot{x} =
  \begin{bmatrix}
    \dot{p}_{x} \\ \dot{p}_{y} \\ \dot{\theta} \\ \frac{1}{m}(u_{1} + u_{2})\sin(\theta) \\ \frac{1}{m}(u_{1} + u_{2})\cos(\theta) - g \\ \frac{\ell}{2J}(u_{2} - u_{1})
  \end{bmatrix}
\end{align*}
where $m$ is the mass of the quadrotor, $\ell$ is the distance between the
propellers, $g$ is acceleration due to gravity, and $J$ is the moment of
inertia of the quadrotor. We use $m = 1$kg, $\ell = 0.3$m, and
$J = 0.2m\ell^{2}$. The objective of the task is to move the quadrotor from an initial
state $x_0$
at $(-3, 1)$ with zero velocity to a final state $x_f$ at $(3, 1)$ with zero
velocity. This is achieved using the per time-step cost function
$c(x, u) = (x - x_{f})^{T}Q(x - x_{f}) + (u - u_{h})^{T}R(u - u_{h})$
where $u_{h} = [\frac{1}{2}mg, \frac{1}{2}mg]$ are the hover controls. We use a
horizon of $H = 60$ with a step size of $0.025$ for RK4 integration.

%%% Local Variables:
%%% mode: latex
%%% TeX-master: "../main"
%%% End:
