
\chapter{Future Work and Conclusion}
\label{cha:future-work-concl}

This chapter concludes the thesis by laying out directions for future
work. The author has made some progress on some of these directions
while for others, pointers are given to related work so that the
reader can get started.

\section{Future Work}
\label{sec:future-work}

\subsection{A Unified Framework for Planning and Execution using
  Inaccurate Models}
\label{sec:unified-framework}

This thesis has presented two novel algorithms \cmax{} and \cmaxpp{} that update the
behavior of the planner, rather than updating the dynamics of the
model, to allow robots to complete the task despite using an
inaccurate model. \cmax{} enables the planner to stick to the
state-action space regions where the model is accurate and biases it
away from the inaccurately modeled regions. \cmaxpp{}, on the other
hand, learns model-free value estimates for inaccurately modeled
transitions and integrates them into a model-based planning procedure
with the inaccurate model. Both approaches require the inaccurate
model to be optimistic and have task-completeness guarantees. While
our experiments have shown that they work very well empirically, there
are domains where designing ``good'' optimistic models is
difficult. By good, we mean a non-trivial optimistic model (a trivial
optimistic model would be one that predicts any transition executed by
robot would complete the task) that is useful in most state-action
space regions. In such domains, updating the dynamics of the model
might be more efficient even in cases where the true dynamics does not
lie in the model class considered. This thesis has taken preliminary
steps in designing such an efficient model learning algorithm in
Chapter~\ref{CHA:TAML} where we presented \taml{} that performs better
than \cmax{} in domains where we can update the dynamics of the model
through low-dimensional parameterizations.

An important future direction would be to build upon \taml{} to design
efficient model learning algorithms that directly optimize task
performance and enable choosing models that are useful for planning
rather than prediction. Initial work in this direction has been done
in~\cite{grimm2020value, DBLP:journals/corr/abs-2106-10316,
  DBLP:conf/icml/AyoubJSWY20, DBLP:journals/corr/abs-2106-14080,
  DBLP:journals/corr/abs-2106-03273,
  DBLP:journals/corr/abs-2110-02758}.
Given such algorithms, the author envisons a
unified framework for planning and execution where the robot, at every
time step, chooses to either update the dynamics of the model from
executions (using algorithms like \taml{}) or updates the behavior of
the planner (using algorithms like \cmax{} and \cmaxpp{}.) This allows
us to combine the advantages of both family of algorithms while
retaining task completeness guarantees. One viable way of implementing
this would be by using the multi-heuristic A* (MHA*)
framework~\cite{DBLP:journals/ijrr/AineSNHL16} where we treat each
algorithm that the robot can use as a heuristic that it can follow to
reach the goal. This would involve maintaining a different set of
cost-to-go estimates for \taml{}, \cmax{} and \cmaxpp{}. Intuitively,
we prefer \cmax{} as it does not waste executions learning dynamics or
learning model-free value estimates and quickly finds an alternative
path. To encode this preference, we can design an anytime algorithm
similar to \acmaxpp{} (in Section~\ref{sec:adaptive}) where if the
cost-to-go following \cmax{} is not too worse compared to that of
\taml{} and \cmaxpp{}, we follow \cmax{}. Else, if the cost-to-go
following \cmaxpp{} is not too far from that of \taml{}, then we
follow \cmaxpp{}. If neither of those are true, then we follow
\taml{}. This encodes the preference that avoiding inaccurately
modeled transitions is easier to learn than model-free value
estimates which is easier to learn than the model dynamics. The goal is to create a unified
framework where the robot, during the course of its execution,
intelligently switches between (a) learning the true dynamics, (b)
learning a model-free value estimate, or (c)
biasing the planner away from an inaccurately modeled
transition to guarantee task completeness while reducing
the amount of real-world experience required.


\subsection{Online Model Learning with Misspecified Model Classes}
\label{sec:online-model-learn}

While \taml{} was a first step in the direction of online model
learning with misspecified model classes where we directly optimize
task performance rather than prediction error, the author believes
there is still a long way to go in this direction. Our main motivation
for this comes from the simulation lemma, which was first introduced
in~\cite{DBLP:journals/ml/KearnsS02}, and can be reformulated in the
undiscounted deterministic dynamics setting as follows:
\begin{lemma}[Undiscounted Deterministic Dynamics Simulation Lemma]
  Let $M, M'$ be two Markov Decision Processes with the same cost
  function. If we have a fixed 
  start state $s_0$, a deterministic policy $\pi:\statespace
  \rightarrow \actionspace$, and $M, M'$ have deterministic dynamics
  $f, f': \statespace \times \actionspace \rightarrow
  \statespace$. Then we have,
  \begin{align}
    \label{eq:23}
    J_M(\pi) &= J_{M'}(\pi) + \sum_{t=0}^\infty c(s_t^M, \pi(s_t^M)) +
               V_{M'}^\pi(s_{t+1}^M) - V_{M'}^\pi(s_t^M) \\
    &= J_{M'}(\pi) + \sum_{t=0}^\infty V_{M'}^\pi(s_{t+1}^M) -
      V^\pi_{M'}(f'(s_t^M, \pi(s_t^M)))
  \end{align}
  where $s_0^M = s_0$ and $s_t^M = f(s_{t-1}^M, \pi(s_{t-1}^M))$.
\end{lemma}

In the case where $M$ is the real world, and $M'$ is any dynamical
model that we consider, the above lemma states that the performance of
any policy $\pi$ in the real world $M$ is equal to the sum of the performance of the
policy in the model $M'$ and the \textit{model advantages} at each time step $V_{M'}^\pi(s_{t+1}^M) -
      V^\pi_{M'}(f'(s_t^M, \pi(s_t^M)))$. Thus, in order to find a
      model $M'$ that captures the performance of a policy as the same
      as that of its performance in the real world, we need to
      minimize model advantages. However, most existing works that
      perform maximum likelihood learning do not consider this
      objective function~\cite{DBLP:journals/arc/Ljung10,
        DBLP:conf/icml/AbbeelN05, DBLP:conf/icml/RossB12, 
  DBLP:journals/corr/abs-1907-02057} and instead use a prediction
error loss. For example, \cite{DBLP:conf/icml/RossB12} present a simple iterative
approach for agnostic system identification with strong guarantees
that do not scale with the size of the MDP when given access to a good
exploration distribution. The approach is very simple to implement and
iterates between collecting new data about the real world $M$ by executing
a good policy under the current model $M'$ as well as by sampling from
the exploration distribution, and updating the model with the new
data. The model is updated by minimizing negative log likelihood of
the data under the model.


To understand why prediction error or maximum likelihood objective
makes sense, let us take a
closer look at the model advantages:
\begin{align*}
  V_{M'}^\pi(s_{t+1}^M) - V^\pi_{M'}(f'(s_t^M, \pi(s_t^M))) &\leq
                                                              L\|s_{t+1}^M - f'(s_t^M, \pi(s_t^M))\| \\
  &\leq L\|f(s_t^M, \pi(s_t^M)) - f'(s_t^M, \pi(s_t^M))\|
\end{align*}
where we assumed that the value function of policy $\pi$ in the model
$M'$ is $L$-lipschitz (any bounded function on a bounded domain is
lipschitz.) Thus, instead of optimizing the model advantages one can
optimize the prediction error which is an upper bound on the
model advantage~\cite{DBLP:conf/icml/RossB12}. However, this can be a
very weak upper bound resulting in a high sample complexity
requirement.

There are two ways to tackle this: 1) directly optimize $J_{M}(\pi)$,
or 2) optimize the model advantages instead of prediction error. RBMS
and \taml{} take the first way by directly optimizing $J_M(\pi)$,
i.e. the performance of the policy in the real world $M$. The policy
class is parameterized by the model class (and the application of
planner $P$) and the performance of policy in $M$ is computed through
an off-policy evaluation procedure. While this works for simple
domains, off-policy evaluation is not always reliable as the data is
collected under a different policy than the policy that is being
evaluated resulting in a high variance estimate.

Another option is to take the second
way. \cite{DBLP:conf/aistats/VoloshinJY21} take this approach in the
offline setting where given an offline collected dataset $\mathcal{D}$
they find a model $M'$ that minimizes the model advantages as
evaluated on $\mathcal{D}$. Once they find the best model in the model
class, they use it for planning to obtain the policy that is then used
for execution in the real world $M$. While this works well in offline
settings, the online version would face similar difficulties as RBMS
where the online collected dataset $\mathcal{D}$ might not have good
coverage and can be highly correlated. Thus, there is a need for an
online model learning algorithm that optimizes model advantages.

The author envisions an online iterative approach similar
to~\cite{DBLP:conf/icml/RossB12} where the model is updated with the
new data by minimizing model advantages rather than minimizing
negative log-likelihood. The advantage of such an approach is evident
in domains where there are no models that capture the true dynamics
exactly in the model class (a.k.a misspecified) but there are several
models that are useful for planning. Using model advantages, instead
of prediction error, allows us to distinguish models that are useful
for planning from models that are good at capturing true dynamics.

\subsection{Extending CMAX and CMAX++ to Stochastic Dynamics}
\label{sec:extend-cmax-cmaxpp}

Most robotics systems are described using deterministic
dynamics. However, when we move to robotics with complex dynamics
(such as contact, friction etc.) we cannot hope to capture everything
that is relevant to dynamics within the state description. This leads
to partial observability. An easy way to tackle this is to formulate
the dynamics as stochastic where the stochasticity results from the
unobserved part of the state.

So far, in this thesis, we have restricted ourselves to deterministic
dynamics (with the exception of Chapter~\ref{CHA:ARS}) for ease of
exposition and because most of the domains we dealt with had simple
dynamics. A natural next step would be to extend the algorithms
introduced, such as \cmax{} and \cmaxpp{}, to systems that are
described using stochastic dynamics. The major component that is
needed for such an extension would be an optimal planner that can work
with stochastic dynamics. The planners, such as RTAA* and LRTA*, are
restricted to deterministic dynamics. The author predicts that we can
use general MDP planners such as value iteration, policy
iteration~\cite{sutton1998introduction} for such purposes. There are
also more domain-specific planners that work with stochastic dynamics
such as stochastic motion
roadmaps~\cite{DBLP:conf/rss/AlterovitzSG07}, stochastic extended
LQR~\cite{DBLP:journals/tase/SunBA16}, and stochastic ensemble
simulation planning~\cite{DBLP:conf/iros/ChiangRT16}, where the
general idea is to maximize the probability of task success given
stochastic dynamics.

In addition to using an optimal planner that can work with stochastic
dynamics, we also need to change how a transition is classified as
inaccurately modeled in \cmax{} and \cmaxpp{}. Rather than having a
hard decision that is made the first time we execute the transition,
we will have to maintain uncertainty estimates on how confident we are
that the transition is not modeled accurately. This might incur an
additional cost of executing the same transition more than once to
classify it as inaccurate with certainty. Similar approaches have been
taken in~\cite{DBLP:conf/nips/KidambiRNJ20,
  DBLP:conf/nips/YuTYEZLFM20} to classify any transition 
as being inaccurately modeled. Once we classify a transition as
inaccurate, we can proceed with penalization in \cmax{} and learning a
model-free value estimate in \cmaxpp{}.

Another challenge in extending to stochastic dynamics would be
achieving the task completeness guarantees for the stochastic versions
of \cmax{} and \cmaxpp{}. The author envisions that by making modeling
assumptions on the stochasticity of the dynamics, and using
well-calibrated uncertainty estimates can result in probabilistic task
completeness guarantees. However, this needs to be explored before
further comments can be made.

\subsection{Finite Data Performance Analysis}
\label{sec:finite-data-perf}

Our performance analysis in Chapter~\ref{CHA:ILC} analyzed the
performance of the final controller that \ilc{} converged to. However,
to converge to this controller, ilc{} might theoretically require a
very large amount of rollouts in the real world leading to a large
number of executions. While this analysis is useful in understanding
what sort of improvements we can expect over naively using the
inaccurate models, it is not realistic of the performance we can
expect given a finite amount of rollouts. Having a more fine grained
performance analysis that tracks the quality of the controller as a
function of both modeling error and the number of rollouts performed
so far would be much more beneficial in understanding the usefulness
of approaches like \cmax{} and \cmaxpp{}.

\section{Conclusion}
\label{sec:conclusion-1}

This thesis takes the first steps in the study of algorithms that
achieve planning and execution using inaccurate models by updating the
behavior of the planner, instead of updating the dynamics of the
model. We present two novel algorithms, \cmax{} and \cmaxpp{}, that
fit this category and also have provable guarantees on task
completeness, i.e. given enough time the planner is bound to find a
path to a goal state. In addition to our algorithmic contributions,
this thesis also presents a performance analysis of such methods in a
simple continuous linearized system setting with quadratic costs. Our
analysis highlights the pitfalls of naively using inaccurate models
for planning and control, and identifies a significant improvement in
performance that can be achieved by updating the behavior of the
planner in this setting. Finally, the thesis takes a small step in the
alternative paradigm of updating the dynamics of the model by
proposing a novel task-aware model learning algorithm \toms{} that updates the dynamics to
directly optimize task performance rather than prediction error in an
online setting where the robot has no access to resets. We conclude
the thesis by pointing out several directions for future work
convincing the reader that there are still a bountiful of exciting
challenges that remain to be solved in this space.

%%% Local Variables:
%%% mode: latex
%%% TeX-master: "../main"
%%% End:
